# This file was automatically generated by SWIG (http://www.swig.org).
# Version 1.3.36
#
# Don't modify this file, modify the SWIG interface instead.
# This file is compatible with both classic and new-style classes.

"""
The `Classifier` module gathers all classifiers available in the SHOGUN toolkit.
"""

import _Classifier
import new
new_instancemethod = new.instancemethod
try:
    _swig_property = property
except NameError:
    pass # Python < 2.2 doesn't have 'property'.
def _swig_setattr_nondynamic(self,class_type,name,value,static=1):
    if (name == "thisown"): return self.this.own(value)
    if (name == "this"):
        if type(value).__name__ == 'PySwigObject':
            self.__dict__[name] = value
            return
    method = class_type.__swig_setmethods__.get(name,None)
    if method: return method(self,value)
    if (not static) or hasattr(self,name):
        self.__dict__[name] = value
    else:
        raise AttributeError("You cannot add attributes to %s" % self)

def _swig_setattr(self,class_type,name,value):
    return _swig_setattr_nondynamic(self,class_type,name,value,0)

def _swig_getattr(self,class_type,name):
    if (name == "thisown"): return self.this.own()
    method = class_type.__swig_getmethods__.get(name,None)
    if method: return method(self)
    raise AttributeError,name

def _swig_repr(self):
    try: strthis = "proxy of " + self.this.__repr__()
    except: strthis = ""
    return "<%s.%s; %s >" % (self.__class__.__module__, self.__class__.__name__, strthis,)

import types
try:
    _object = types.ObjectType
    _newclass = 1
except AttributeError:
    class _object : pass
    _newclass = 0
del types


class ShogunException(_object):
    """Proxy of C++ ShogunException class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, ShogunException, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, ShogunException, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """__init__(self, str) -> ShogunException"""
        this = _Classifier.new_ShogunException(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_ShogunException
    __del__ = lambda self : None;
    def get_exception_string(*args):
        """get_exception_string(self) -> str"""
        return _Classifier.ShogunException_get_exception_string(*args)

ShogunException_swigregister = _Classifier.ShogunException_swigregister
ShogunException_swigregister(ShogunException)

MSG_GCDEBUG = _Classifier.MSG_GCDEBUG
MSG_DEBUG = _Classifier.MSG_DEBUG
MSG_INFO = _Classifier.MSG_INFO
MSG_NOTICE = _Classifier.MSG_NOTICE
MSG_WARN = _Classifier.MSG_WARN
MSG_ERROR = _Classifier.MSG_ERROR
MSG_CRITICAL = _Classifier.MSG_CRITICAL
MSG_ALERT = _Classifier.MSG_ALERT
MSG_EMERGENCY = _Classifier.MSG_EMERGENCY
MSG_MESSAGEONLY = _Classifier.MSG_MESSAGEONLY
class IO(_object):
    """Proxy of C++ IO class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, IO, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, IO, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> IO
        __init__(self, orig) -> IO
        """
        this = _Classifier.new_IO(*args)
        try: self.this.append(this)
        except: self.this = this
    def set_loglevel(*args):
        """set_loglevel(self, level)"""
        return _Classifier.IO_set_loglevel(*args)

    def get_loglevel(*args):
        """get_loglevel(self) -> EMessageType"""
        return _Classifier.IO_get_loglevel(*args)

    def get_show_progress(*args):
        """get_show_progress(self) -> bool"""
        return _Classifier.IO_get_show_progress(*args)

    def get_show_file_and_line(*args):
        """get_show_file_and_line(self) -> bool"""
        return _Classifier.IO_get_show_file_and_line(*args)

    def message(*args):
        """message(self, prio, file, line, fmt, ?)"""
        return _Classifier.IO_message(*args)

    def progress(*args):
        """
        progress(self, current_val, min_val=0.0, max_val=1.0, decimals=1, 
            prefix="PROGRESS:\t")
        progress(self, current_val, min_val=0.0, max_val=1.0, decimals=1)
        progress(self, current_val, min_val=0.0, max_val=1.0)
        progress(self, current_val, min_val=0.0)
        progress(self, current_val)
        """
        return _Classifier.IO_progress(*args)

    def absolute_progress(*args):
        """
        absolute_progress(self, current_val, val, min_val=0.0, max_val=1.0, decimals=1, 
            prefix="PROGRESS:\t")
        absolute_progress(self, current_val, val, min_val=0.0, max_val=1.0, decimals=1)
        absolute_progress(self, current_val, val, min_val=0.0, max_val=1.0)
        absolute_progress(self, current_val, val, min_val=0.0)
        absolute_progress(self, current_val, val)
        """
        return _Classifier.IO_absolute_progress(*args)

    def done(*args):
        """done(self)"""
        return _Classifier.IO_done(*args)

    def not_implemented(*args):
        """not_implemented(self, file, line)"""
        return _Classifier.IO_not_implemented(*args)

    def deprecated(*args):
        """deprecated(self, file, line)"""
        return _Classifier.IO_deprecated(*args)

    def buffered_message(*args):
        """buffered_message(self, prio, fmt, ?)"""
        return _Classifier.IO_buffered_message(*args)

    def skip_spaces(*args):
        """skip_spaces(str) -> str"""
        return _Classifier.IO_skip_spaces(*args)

    if _newclass:skip_spaces = staticmethod(skip_spaces)
    __swig_getmethods__["skip_spaces"] = lambda x: skip_spaces
    def skip_blanks(*args):
        """skip_blanks(str) -> str"""
        return _Classifier.IO_skip_blanks(*args)

    if _newclass:skip_blanks = staticmethod(skip_blanks)
    __swig_getmethods__["skip_blanks"] = lambda x: skip_blanks
    def get_target(*args):
        """get_target(self) -> FILE"""
        return _Classifier.IO_get_target(*args)

    def set_target(*args):
        """set_target(self, target)"""
        return _Classifier.IO_set_target(*args)

    def set_target_to_stderr(*args):
        """set_target_to_stderr(self)"""
        return _Classifier.IO_set_target_to_stderr(*args)

    def set_target_to_stdout(*args):
        """set_target_to_stdout(self)"""
        return _Classifier.IO_set_target_to_stdout(*args)

    def enable_progress(*args):
        """enable_progress(self)"""
        return _Classifier.IO_enable_progress(*args)

    def disable_progress(*args):
        """disable_progress(self)"""
        return _Classifier.IO_disable_progress(*args)

    def enable_file_and_line(*args):
        """enable_file_and_line(self)"""
        return _Classifier.IO_enable_file_and_line(*args)

    def disable_file_and_line(*args):
        """disable_file_and_line(self)"""
        return _Classifier.IO_disable_file_and_line(*args)

    def set_dirname(*args):
        """set_dirname(dirname)"""
        return _Classifier.IO_set_dirname(*args)

    if _newclass:set_dirname = staticmethod(set_dirname)
    __swig_getmethods__["set_dirname"] = lambda x: set_dirname
    def concat_filename(*args):
        """concat_filename(filename) -> str"""
        return _Classifier.IO_concat_filename(*args)

    if _newclass:concat_filename = staticmethod(concat_filename)
    __swig_getmethods__["concat_filename"] = lambda x: concat_filename
    def filter(*args):
        """filter(d) -> int"""
        return _Classifier.IO_filter(*args)

    if _newclass:filter = staticmethod(filter)
    __swig_getmethods__["filter"] = lambda x: filter
    def ref(*args):
        """ref(self) ->  int"""
        return _Classifier.IO_ref(*args)

    def ref_count(*args):
        """ref_count(self) ->  int"""
        return _Classifier.IO_ref_count(*args)

    def unref(*args):
        """unref(self) ->  int"""
        return _Classifier.IO_unref(*args)

    def get_name(*args):
        """get_name(self) -> str"""
        return _Classifier.IO_get_name(*args)

    __swig_destroy__ = _Classifier.delete_IO
    __del__ = lambda self : None;
IO_swigregister = _Classifier.IO_swigregister
IO_swigregister(IO)
cvar = _Classifier.cvar

def IO_skip_spaces(*args):
  """IO_skip_spaces(str) -> str"""
  return _Classifier.IO_skip_spaces(*args)

def IO_skip_blanks(*args):
  """IO_skip_blanks(str) -> str"""
  return _Classifier.IO_skip_blanks(*args)

def IO_set_dirname(*args):
  """IO_set_dirname(dirname)"""
  return _Classifier.IO_set_dirname(*args)

def IO_concat_filename(*args):
  """IO_concat_filename(filename) -> str"""
  return _Classifier.IO_concat_filename(*args)

def IO_filter(*args):
  """IO_filter(d) -> int"""
  return _Classifier.IO_filter(*args)

class SGObject(_object):
    """Proxy of C++ SGObject class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, SGObject, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, SGObject, name)
    def __init__(self, *args, **kwargs): raise AttributeError, "No constructor defined"
    __repr__ = _swig_repr
    __swig_destroy__ = _Classifier.delete_SGObject
    __del__ = lambda self : None;
    def ref(*args):
        """ref(self) ->  int"""
        return _Classifier.SGObject_ref(*args)

    def ref_count(*args):
        """ref_count(self) ->  int"""
        return _Classifier.SGObject_ref_count(*args)

    def unref(*args):
        """unref(self) ->  int"""
        return _Classifier.SGObject_unref(*args)

    def get_name(*args):
        """get_name(self) -> str"""
        return _Classifier.SGObject_get_name(*args)

    __swig_setmethods__["io"] = _Classifier.SGObject_io_set
    __swig_getmethods__["io"] = _Classifier.SGObject_io_get
    if _newclass:io = _swig_property(_Classifier.SGObject_io_get, _Classifier.SGObject_io_set)
    __swig_setmethods__["parallel"] = _Classifier.SGObject_parallel_set
    __swig_getmethods__["parallel"] = _Classifier.SGObject_parallel_get
    if _newclass:parallel = _swig_property(_Classifier.SGObject_parallel_get, _Classifier.SGObject_parallel_set)
    __swig_setmethods__["version"] = _Classifier.SGObject_version_set
    __swig_getmethods__["version"] = _Classifier.SGObject_version_get
    if _newclass:version = _swig_property(_Classifier.SGObject_version_get, _Classifier.SGObject_version_set)
SGObject_swigregister = _Classifier.SGObject_swigregister
SGObject_swigregister(SGObject)

class Version(_object):
    """Proxy of C++ Version class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, Version, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, Version, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """__init__(self) -> Version"""
        this = _Classifier.new_Version(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_Version
    __del__ = lambda self : None;
    def print_version(*args):
        """print_version()"""
        return _Classifier.Version_print_version(*args)

    if _newclass:print_version = staticmethod(print_version)
    __swig_getmethods__["print_version"] = lambda x: print_version
    def get_version_extra(*args):
        """get_version_extra() -> str"""
        return _Classifier.Version_get_version_extra(*args)

    if _newclass:get_version_extra = staticmethod(get_version_extra)
    __swig_getmethods__["get_version_extra"] = lambda x: get_version_extra
    def get_version_release(*args):
        """get_version_release() -> str"""
        return _Classifier.Version_get_version_release(*args)

    if _newclass:get_version_release = staticmethod(get_version_release)
    __swig_getmethods__["get_version_release"] = lambda x: get_version_release
    def get_version_revision(*args):
        """get_version_revision() ->  int"""
        return _Classifier.Version_get_version_revision(*args)

    if _newclass:get_version_revision = staticmethod(get_version_revision)
    __swig_getmethods__["get_version_revision"] = lambda x: get_version_revision
    def get_version_year(*args):
        """get_version_year() ->  int"""
        return _Classifier.Version_get_version_year(*args)

    if _newclass:get_version_year = staticmethod(get_version_year)
    __swig_getmethods__["get_version_year"] = lambda x: get_version_year
    def get_version_month(*args):
        """get_version_month() ->  int"""
        return _Classifier.Version_get_version_month(*args)

    if _newclass:get_version_month = staticmethod(get_version_month)
    __swig_getmethods__["get_version_month"] = lambda x: get_version_month
    def get_version_day(*args):
        """get_version_day() ->  int"""
        return _Classifier.Version_get_version_day(*args)

    if _newclass:get_version_day = staticmethod(get_version_day)
    __swig_getmethods__["get_version_day"] = lambda x: get_version_day
    def get_version_hour(*args):
        """get_version_hour() ->  int"""
        return _Classifier.Version_get_version_hour(*args)

    if _newclass:get_version_hour = staticmethod(get_version_hour)
    __swig_getmethods__["get_version_hour"] = lambda x: get_version_hour
    def get_version_minute(*args):
        """get_version_minute() ->  int"""
        return _Classifier.Version_get_version_minute(*args)

    if _newclass:get_version_minute = staticmethod(get_version_minute)
    __swig_getmethods__["get_version_minute"] = lambda x: get_version_minute
    def get_version_in_minutes(*args):
        """get_version_in_minutes() -> int"""
        return _Classifier.Version_get_version_in_minutes(*args)

    if _newclass:get_version_in_minutes = staticmethod(get_version_in_minutes)
    __swig_getmethods__["get_version_in_minutes"] = lambda x: get_version_in_minutes
    def ref(*args):
        """ref(self) ->  int"""
        return _Classifier.Version_ref(*args)

    def ref_count(*args):
        """ref_count(self) ->  int"""
        return _Classifier.Version_ref_count(*args)

    def unref(*args):
        """unref(self) ->  int"""
        return _Classifier.Version_unref(*args)

Version_swigregister = _Classifier.Version_swigregister
Version_swigregister(Version)

def Version_print_version(*args):
  """Version_print_version()"""
  return _Classifier.Version_print_version(*args)

def Version_get_version_extra(*args):
  """Version_get_version_extra() -> str"""
  return _Classifier.Version_get_version_extra(*args)

def Version_get_version_release(*args):
  """Version_get_version_release() -> str"""
  return _Classifier.Version_get_version_release(*args)

def Version_get_version_revision(*args):
  """Version_get_version_revision() ->  int"""
  return _Classifier.Version_get_version_revision(*args)

def Version_get_version_year(*args):
  """Version_get_version_year() ->  int"""
  return _Classifier.Version_get_version_year(*args)

def Version_get_version_month(*args):
  """Version_get_version_month() ->  int"""
  return _Classifier.Version_get_version_month(*args)

def Version_get_version_day(*args):
  """Version_get_version_day() ->  int"""
  return _Classifier.Version_get_version_day(*args)

def Version_get_version_hour(*args):
  """Version_get_version_hour() ->  int"""
  return _Classifier.Version_get_version_hour(*args)

def Version_get_version_minute(*args):
  """Version_get_version_minute() ->  int"""
  return _Classifier.Version_get_version_minute(*args)

def Version_get_version_in_minutes(*args):
  """Version_get_version_in_minutes() -> int"""
  return _Classifier.Version_get_version_in_minutes(*args)

class Parallel(_object):
    """Proxy of C++ Parallel class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, Parallel, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, Parallel, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> Parallel
        __init__(self, orig) -> Parallel
        """
        this = _Classifier.new_Parallel(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_Parallel
    __del__ = lambda self : None;
    def get_num_cpus(*args):
        """get_num_cpus(self) ->  int"""
        return _Classifier.Parallel_get_num_cpus(*args)

    def set_num_threads(*args):
        """set_num_threads(self, n)"""
        return _Classifier.Parallel_set_num_threads(*args)

    def get_num_threads(*args):
        """get_num_threads(self) ->  int"""
        return _Classifier.Parallel_get_num_threads(*args)

    def ref(*args):
        """ref(self) ->  int"""
        return _Classifier.Parallel_ref(*args)

    def ref_count(*args):
        """ref_count(self) ->  int"""
        return _Classifier.Parallel_ref_count(*args)

    def unref(*args):
        """unref(self) ->  int"""
        return _Classifier.Parallel_unref(*args)

Parallel_swigregister = _Classifier.Parallel_swigregister
Parallel_swigregister(Parallel)

class PySwigIterator(_object):
    """Proxy of C++ PySwigIterator class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, PySwigIterator, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, PySwigIterator, name)
    def __init__(self, *args, **kwargs): raise AttributeError, "No constructor defined"
    __repr__ = _swig_repr
    __swig_destroy__ = _Classifier.delete_PySwigIterator
    __del__ = lambda self : None;
    def value(*args):
        """value(self) -> PyObject"""
        return _Classifier.PySwigIterator_value(*args)

    def incr(*args):
        """
        incr(self, n=1) -> PySwigIterator
        incr(self) -> PySwigIterator
        """
        return _Classifier.PySwigIterator_incr(*args)

    def decr(*args):
        """
        decr(self, n=1) -> PySwigIterator
        decr(self) -> PySwigIterator
        """
        return _Classifier.PySwigIterator_decr(*args)

    def distance(*args):
        """distance(self, x) -> ptrdiff_t"""
        return _Classifier.PySwigIterator_distance(*args)

    def equal(*args):
        """equal(self, x) -> bool"""
        return _Classifier.PySwigIterator_equal(*args)

    def copy(*args):
        """copy(self) -> PySwigIterator"""
        return _Classifier.PySwigIterator_copy(*args)

    def next(*args):
        """next(self) -> PyObject"""
        return _Classifier.PySwigIterator_next(*args)

    def previous(*args):
        """previous(self) -> PyObject"""
        return _Classifier.PySwigIterator_previous(*args)

    def advance(*args):
        """advance(self, n) -> PySwigIterator"""
        return _Classifier.PySwigIterator_advance(*args)

    def __eq__(*args):
        """__eq__(self, x) -> bool"""
        return _Classifier.PySwigIterator___eq__(*args)

    def __ne__(*args):
        """__ne__(self, x) -> bool"""
        return _Classifier.PySwigIterator___ne__(*args)

    def __iadd__(*args):
        """__iadd__(self, n) -> PySwigIterator"""
        return _Classifier.PySwigIterator___iadd__(*args)

    def __isub__(*args):
        """__isub__(self, n) -> PySwigIterator"""
        return _Classifier.PySwigIterator___isub__(*args)

    def __add__(*args):
        """__add__(self, n) -> PySwigIterator"""
        return _Classifier.PySwigIterator___add__(*args)

    def __sub__(*args):
        """
        __sub__(self, n) -> PySwigIterator
        __sub__(self, x) -> ptrdiff_t
        """
        return _Classifier.PySwigIterator___sub__(*args)

    def __iter__(self): return self
PySwigIterator_swigregister = _Classifier.PySwigIterator_swigregister
PySwigIterator_swigregister(PySwigIterator)

class IntVector(_object):
    """Proxy of C++ IntVector class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, IntVector, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, IntVector, name)
    __repr__ = _swig_repr
    def iterator(*args):
        """iterator(self, PYTHON_SELF) -> PySwigIterator"""
        return _Classifier.IntVector_iterator(*args)

    def __iter__(self): return self.iterator()
    def __nonzero__(*args):
        """__nonzero__(self) -> bool"""
        return _Classifier.IntVector___nonzero__(*args)

    def __len__(*args):
        """__len__(self) -> std::vector<(int)>::size_type"""
        return _Classifier.IntVector___len__(*args)

    def pop(*args):
        """pop(self) -> std::vector<(int)>::value_type"""
        return _Classifier.IntVector_pop(*args)

    def __getslice__(*args):
        """__getslice__(self, i, j) -> IntVector"""
        return _Classifier.IntVector___getslice__(*args)

    def __setslice__(*args):
        """__setslice__(self, i, j, v)"""
        return _Classifier.IntVector___setslice__(*args)

    def __delslice__(*args):
        """__delslice__(self, i, j)"""
        return _Classifier.IntVector___delslice__(*args)

    def __delitem__(*args):
        """__delitem__(self, i)"""
        return _Classifier.IntVector___delitem__(*args)

    def __getitem__(*args):
        """__getitem__(self, i) -> std::vector<(int)>::value_type"""
        return _Classifier.IntVector___getitem__(*args)

    def __setitem__(*args):
        """__setitem__(self, i, x)"""
        return _Classifier.IntVector___setitem__(*args)

    def append(*args):
        """append(self, x)"""
        return _Classifier.IntVector_append(*args)

    def empty(*args):
        """empty(self) -> bool"""
        return _Classifier.IntVector_empty(*args)

    def size(*args):
        """size(self) -> std::vector<(int)>::size_type"""
        return _Classifier.IntVector_size(*args)

    def clear(*args):
        """clear(self)"""
        return _Classifier.IntVector_clear(*args)

    def swap(*args):
        """swap(self, v)"""
        return _Classifier.IntVector_swap(*args)

    def get_allocator(*args):
        """get_allocator(self) -> std::vector<(int)>::allocator_type"""
        return _Classifier.IntVector_get_allocator(*args)

    def begin(*args):
        """begin(self) -> std::vector<(int)>::const_iterator"""
        return _Classifier.IntVector_begin(*args)

    def end(*args):
        """end(self) -> std::vector<(int)>::const_iterator"""
        return _Classifier.IntVector_end(*args)

    def rbegin(*args):
        """rbegin(self) -> std::vector<(int)>::const_reverse_iterator"""
        return _Classifier.IntVector_rbegin(*args)

    def rend(*args):
        """rend(self) -> std::vector<(int)>::const_reverse_iterator"""
        return _Classifier.IntVector_rend(*args)

    def pop_back(*args):
        """pop_back(self)"""
        return _Classifier.IntVector_pop_back(*args)

    def erase(*args):
        """
        erase(self, pos) -> std::vector<(int)>::iterator
        erase(self, first, last) -> std::vector<(int)>::iterator
        """
        return _Classifier.IntVector_erase(*args)

    def __init__(self, *args): 
        """
        __init__(self) -> IntVector
        __init__(self, ?) -> IntVector
        __init__(self, size) -> IntVector
        __init__(self, size, value) -> IntVector
        """
        this = _Classifier.new_IntVector(*args)
        try: self.this.append(this)
        except: self.this = this
    def push_back(*args):
        """push_back(self, x)"""
        return _Classifier.IntVector_push_back(*args)

    def front(*args):
        """front(self) -> std::vector<(int)>::value_type"""
        return _Classifier.IntVector_front(*args)

    def back(*args):
        """back(self) -> std::vector<(int)>::value_type"""
        return _Classifier.IntVector_back(*args)

    def assign(*args):
        """assign(self, n, x)"""
        return _Classifier.IntVector_assign(*args)

    def resize(*args):
        """
        resize(self, new_size)
        resize(self, new_size, x)
        """
        return _Classifier.IntVector_resize(*args)

    def insert(*args):
        """
        insert(self, pos, x) -> std::vector<(int)>::iterator
        insert(self, pos, n, x)
        """
        return _Classifier.IntVector_insert(*args)

    def reserve(*args):
        """reserve(self, n)"""
        return _Classifier.IntVector_reserve(*args)

    def capacity(*args):
        """capacity(self) -> std::vector<(int)>::size_type"""
        return _Classifier.IntVector_capacity(*args)

    __swig_destroy__ = _Classifier.delete_IntVector
    __del__ = lambda self : None;
IntVector_swigregister = _Classifier.IntVector_swigregister
IntVector_swigregister(IntVector)

class DoubleVector(_object):
    """Proxy of C++ DoubleVector class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, DoubleVector, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, DoubleVector, name)
    __repr__ = _swig_repr
    def iterator(*args):
        """iterator(self, PYTHON_SELF) -> PySwigIterator"""
        return _Classifier.DoubleVector_iterator(*args)

    def __iter__(self): return self.iterator()
    def __nonzero__(*args):
        """__nonzero__(self) -> bool"""
        return _Classifier.DoubleVector___nonzero__(*args)

    def __len__(*args):
        """__len__(self) -> std::vector<(double)>::size_type"""
        return _Classifier.DoubleVector___len__(*args)

    def pop(*args):
        """pop(self) -> std::vector<(double)>::value_type"""
        return _Classifier.DoubleVector_pop(*args)

    def __getslice__(*args):
        """__getslice__(self, i, j) -> DoubleVector"""
        return _Classifier.DoubleVector___getslice__(*args)

    def __setslice__(*args):
        """__setslice__(self, i, j, v)"""
        return _Classifier.DoubleVector___setslice__(*args)

    def __delslice__(*args):
        """__delslice__(self, i, j)"""
        return _Classifier.DoubleVector___delslice__(*args)

    def __delitem__(*args):
        """__delitem__(self, i)"""
        return _Classifier.DoubleVector___delitem__(*args)

    def __getitem__(*args):
        """__getitem__(self, i) -> std::vector<(double)>::value_type"""
        return _Classifier.DoubleVector___getitem__(*args)

    def __setitem__(*args):
        """__setitem__(self, i, x)"""
        return _Classifier.DoubleVector___setitem__(*args)

    def append(*args):
        """append(self, x)"""
        return _Classifier.DoubleVector_append(*args)

    def empty(*args):
        """empty(self) -> bool"""
        return _Classifier.DoubleVector_empty(*args)

    def size(*args):
        """size(self) -> std::vector<(double)>::size_type"""
        return _Classifier.DoubleVector_size(*args)

    def clear(*args):
        """clear(self)"""
        return _Classifier.DoubleVector_clear(*args)

    def swap(*args):
        """swap(self, v)"""
        return _Classifier.DoubleVector_swap(*args)

    def get_allocator(*args):
        """get_allocator(self) -> std::vector<(double)>::allocator_type"""
        return _Classifier.DoubleVector_get_allocator(*args)

    def begin(*args):
        """begin(self) -> std::vector<(double)>::const_iterator"""
        return _Classifier.DoubleVector_begin(*args)

    def end(*args):
        """end(self) -> std::vector<(double)>::const_iterator"""
        return _Classifier.DoubleVector_end(*args)

    def rbegin(*args):
        """rbegin(self) -> std::vector<(double)>::const_reverse_iterator"""
        return _Classifier.DoubleVector_rbegin(*args)

    def rend(*args):
        """rend(self) -> std::vector<(double)>::const_reverse_iterator"""
        return _Classifier.DoubleVector_rend(*args)

    def pop_back(*args):
        """pop_back(self)"""
        return _Classifier.DoubleVector_pop_back(*args)

    def erase(*args):
        """
        erase(self, pos) -> std::vector<(double)>::iterator
        erase(self, first, last) -> std::vector<(double)>::iterator
        """
        return _Classifier.DoubleVector_erase(*args)

    def __init__(self, *args): 
        """
        __init__(self) -> DoubleVector
        __init__(self, ?) -> DoubleVector
        __init__(self, size) -> DoubleVector
        __init__(self, size, value) -> DoubleVector
        """
        this = _Classifier.new_DoubleVector(*args)
        try: self.this.append(this)
        except: self.this = this
    def push_back(*args):
        """push_back(self, x)"""
        return _Classifier.DoubleVector_push_back(*args)

    def front(*args):
        """front(self) -> std::vector<(double)>::value_type"""
        return _Classifier.DoubleVector_front(*args)

    def back(*args):
        """back(self) -> std::vector<(double)>::value_type"""
        return _Classifier.DoubleVector_back(*args)

    def assign(*args):
        """assign(self, n, x)"""
        return _Classifier.DoubleVector_assign(*args)

    def resize(*args):
        """
        resize(self, new_size)
        resize(self, new_size, x)
        """
        return _Classifier.DoubleVector_resize(*args)

    def insert(*args):
        """
        insert(self, pos, x) -> std::vector<(double)>::iterator
        insert(self, pos, n, x)
        """
        return _Classifier.DoubleVector_insert(*args)

    def reserve(*args):
        """reserve(self, n)"""
        return _Classifier.DoubleVector_reserve(*args)

    def capacity(*args):
        """capacity(self) -> std::vector<(double)>::size_type"""
        return _Classifier.DoubleVector_capacity(*args)

    __swig_destroy__ = _Classifier.delete_DoubleVector
    __del__ = lambda self : None;
DoubleVector_swigregister = _Classifier.DoubleVector_swigregister
DoubleVector_swigregister(DoubleVector)

CT_NONE = _Classifier.CT_NONE
CT_LIGHT = _Classifier.CT_LIGHT
CT_LIBSVM = _Classifier.CT_LIBSVM
CT_LIBSVMONECLASS = _Classifier.CT_LIBSVMONECLASS
CT_LIBSVMMULTICLASS = _Classifier.CT_LIBSVMMULTICLASS
CT_MPD = _Classifier.CT_MPD
CT_GPBT = _Classifier.CT_GPBT
CT_CPLEXSVM = _Classifier.CT_CPLEXSVM
CT_PERCEPTRON = _Classifier.CT_PERCEPTRON
CT_KERNELPERCEPTRON = _Classifier.CT_KERNELPERCEPTRON
CT_LDA = _Classifier.CT_LDA
CT_LPM = _Classifier.CT_LPM
CT_LPBOOST = _Classifier.CT_LPBOOST
CT_KNN = _Classifier.CT_KNN
CT_SVMLIN = _Classifier.CT_SVMLIN
CT_KRR = _Classifier.CT_KRR
CT_GNPPSVM = _Classifier.CT_GNPPSVM
CT_GMNPSVM = _Classifier.CT_GMNPSVM
CT_SUBGRADIENTSVM = _Classifier.CT_SUBGRADIENTSVM
CT_SUBGRADIENTLPM = _Classifier.CT_SUBGRADIENTLPM
CT_SVMPERF = _Classifier.CT_SVMPERF
CT_LIBSVR = _Classifier.CT_LIBSVR
CT_SVRLIGHT = _Classifier.CT_SVRLIGHT
CT_LIBLINEAR = _Classifier.CT_LIBLINEAR
CT_KMEANS = _Classifier.CT_KMEANS
CT_HIERARCHICAL = _Classifier.CT_HIERARCHICAL
CT_SVMOCAS = _Classifier.CT_SVMOCAS
CT_WDSVMOCAS = _Classifier.CT_WDSVMOCAS
CT_SVMSGD = _Classifier.CT_SVMSGD
CT_MKLMULTICLASS = _Classifier.CT_MKLMULTICLASS
CT_MKLCLASSIFICATION = _Classifier.CT_MKLCLASSIFICATION
CT_MKLONECLASS = _Classifier.CT_MKLONECLASS
CT_MKLREGRESSION = _Classifier.CT_MKLREGRESSION
CT_SCATTERSVM = _Classifier.CT_SCATTERSVM
CT_DASVM = _Classifier.CT_DASVM
CT_LARANK = _Classifier.CT_LARANK
ST_AUTO = _Classifier.ST_AUTO
ST_CPLEX = _Classifier.ST_CPLEX
ST_GLPK = _Classifier.ST_GLPK
ST_NEWTON = _Classifier.ST_NEWTON
ST_DIRECT = _Classifier.ST_DIRECT
class Classifier(SGObject):
    """
    A generic classifier interface.

    A classifier takes as input CLabels. Later subclasses may specialize
    the classifier to require labels and a kernel or labels and (real-
    valued) features.

    A classifier needs to override the train() function for training, the
    function classify_example() (optionally classify() to predict on the
    whole set of examples) and the load and save routines.

    C++ includes: Classifier.h 
    """
    __swig_setmethods__ = {}
    for _s in [SGObject]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, Classifier, name, value)
    __swig_getmethods__ = {}
    for _s in [SGObject]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, Classifier, name)
    def __init__(self, *args, **kwargs): raise AttributeError, "No constructor defined"
    __repr__ = _swig_repr
    __swig_destroy__ = _Classifier.delete_Classifier
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.Classifier_train(*args)

    def classify(*args):
        """
        classify(self) -> CLabels
        classify(self, data) -> CLabels

        classify objects

        Parameters:
        -----------

        data:  (test) data to be classified

        classified labels 
        """
        return _Classifier.Classifier_classify(*args)

    def classify_example(*args):
        """
        classify_example(self, num) -> float

        classify one example

        abstract base method

        Parameters:
        -----------

        num:  which example to classify

        infinite float value 
        """
        return _Classifier.Classifier_classify_example(*args)

    def load(*args):
        """
        load(self, srcfile) -> bool

        load Classifier from file

        abstract base method

        Parameters:
        -----------

        srcfile:  file to load from

        failure 
        """
        return _Classifier.Classifier_load(*args)

    def save(*args):
        """
        save(self, dstfile) -> bool

        save Classifier to file

        abstract base method

        Parameters:
        -----------

        dstfile:  file to save to

        failure 
        """
        return _Classifier.Classifier_save(*args)

    def set_labels(*args):
        """
        set_labels(self, lab)

        set labels

        Parameters:
        -----------

        lab:  labels 
        """
        return _Classifier.Classifier_set_labels(*args)

    def get_labels(*args):
        """
        get_labels(self) -> CLabels

        get labels

        labels 
        """
        return _Classifier.Classifier_get_labels(*args)

    def get_label(*args):
        """
        get_label(self, i) -> float

        get one specific label

        Parameters:
        -----------

        i:  index of label to get

        value of label at index i 
        """
        return _Classifier.Classifier_get_label(*args)

    def set_max_train_time(*args):
        """
        set_max_train_time(self, t)

        set maximum training time

        Parameters:
        -----------

        t:  maximimum training time 
        """
        return _Classifier.Classifier_set_max_train_time(*args)

    def get_max_train_time(*args):
        """
        get_max_train_time(self) -> float

        get maximum training time

        maximum training time 
        """
        return _Classifier.Classifier_get_max_train_time(*args)

    def get_classifier_type(*args):
        """
        get_classifier_type(self) -> EClassifierType

        get classifier type

        classifier type NONE 
        """
        return _Classifier.Classifier_get_classifier_type(*args)

    def set_solver_type(*args):
        """
        set_solver_type(self, st)

        set solver type

        Parameters:
        -----------

        st:  solver type 
        """
        return _Classifier.Classifier_set_solver_type(*args)

    def get_solver_type(*args):
        """
        get_solver_type(self) -> ESolverType

        get solver type

        solver 
        """
        return _Classifier.Classifier_get_solver_type(*args)

Classifier_swigregister = _Classifier.Classifier_swigregister
Classifier_swigregister(Classifier)

class CKernelMachine(Classifier):
    """
    A generic KernelMachine interface.

    A kernel machine is defined as \\[ f({\\bf x})=\\sum_{i=0}^{N-1}
    \\alpha_i k({\\bf x}, {\\bf x_i})+b \\]

    where $N$ is the number of training examples $\\alpha_i$ are the
    weights assigned to each training example $k(x,x')$ is the kernel and
    $b$ the bias.

    Using an a-priori choosen kernel, the $\\alpha_i$ and bias are
    determined in a training procedure.

    C++ includes: KernelMachine.h 
    """
    __swig_setmethods__ = {}
    for _s in [Classifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, CKernelMachine, name, value)
    __swig_getmethods__ = {}
    for _s in [Classifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, CKernelMachine, name)
    def __init__(self, *args, **kwargs): raise AttributeError, "No constructor defined"
    __repr__ = _swig_repr
    __swig_destroy__ = _Classifier.delete_CKernelMachine
    __del__ = lambda self : None;
    def set_kernel(*args):
        """
        set_kernel(self, k)

        set kernel

        Parameters:
        -----------

        k:  kernel 
        """
        return _Classifier.CKernelMachine_set_kernel(*args)

    def get_kernel(*args):
        """
        get_kernel(self) -> CKernel

        get kernel

        kernel 
        """
        return _Classifier.CKernelMachine_get_kernel(*args)

    def set_batch_computation_enabled(*args):
        """
        set_batch_computation_enabled(self, enable)

        set batch computation enabled

        Parameters:
        -----------

        enable:  if batch computation shall be enabled 
        """
        return _Classifier.CKernelMachine_set_batch_computation_enabled(*args)

    def get_batch_computation_enabled(*args):
        """
        get_batch_computation_enabled(self) -> bool

        check if batch computation is enabled

        if batch computation is enabled 
        """
        return _Classifier.CKernelMachine_get_batch_computation_enabled(*args)

    def set_linadd_enabled(*args):
        """
        set_linadd_enabled(self, enable)

        set linadd enabled

        Parameters:
        -----------

        enable:  if linadd shall be enabled 
        """
        return _Classifier.CKernelMachine_set_linadd_enabled(*args)

    def get_linadd_enabled(*args):
        """
        get_linadd_enabled(self) -> bool

        check if linadd is enabled

        if linadd is enabled 
        """
        return _Classifier.CKernelMachine_get_linadd_enabled(*args)

    def set_bias_enabled(*args):
        """
        set_bias_enabled(self, enable_bias)

        set state of bias

        Parameters:
        -----------

        enable_bias:  if bias shall be enabled 
        """
        return _Classifier.CKernelMachine_set_bias_enabled(*args)

    def get_bias_enabled(*args):
        """
        get_bias_enabled(self) -> bool

        get state of bias

        state of bias 
        """
        return _Classifier.CKernelMachine_get_bias_enabled(*args)

    def get_bias(*args):
        """
        get_bias(self) -> float

        get bias

        bias 
        """
        return _Classifier.CKernelMachine_get_bias(*args)

    def set_bias(*args):
        """
        set_bias(self, bias)

        set bias to given value

        Parameters:
        -----------

        bias:  new bias 
        """
        return _Classifier.CKernelMachine_set_bias(*args)

    def get_support_vector(*args):
        """
        get_support_vector(self, idx) ->  int

        get support vector at given index

        Parameters:
        -----------

        idx:  index of support vector

        support vector 
        """
        return _Classifier.CKernelMachine_get_support_vector(*args)

    def get_alpha(*args):
        """
        get_alpha(self, idx) -> float

        get alpha at given index

        Parameters:
        -----------

        idx:  index of alpha

        alpha 
        """
        return _Classifier.CKernelMachine_get_alpha(*args)

    def set_support_vector(*args):
        """
        set_support_vector(self, idx, val) -> bool

        set support vector at given index to given value

        Parameters:
        -----------

        idx:  index of support vector

        val:  new value of support vector

        if operation was successful 
        """
        return _Classifier.CKernelMachine_set_support_vector(*args)

    def set_alpha(*args):
        """
        set_alpha(self, idx, val) -> bool

        set alpha at given index to given value

        Parameters:
        -----------

        idx:  index of alpha vector

        val:  new value of alpha vector

        if operation was successful 
        """
        return _Classifier.CKernelMachine_set_alpha(*args)

    def get_num_support_vectors(*args):
        """
        get_num_support_vectors(self) ->  int

        get number of support vectors

        number of support vectors 
        """
        return _Classifier.CKernelMachine_get_num_support_vectors(*args)

    def set_alphas(*args):
        """
        set_alphas(self, alphas)

        set alphas to given values

        Parameters:
        -----------

        alphas:  array with all alphas to set

        d:  number of alphas (== number of support vectors) 
        """
        return _Classifier.CKernelMachine_set_alphas(*args)

    def set_support_vectors(*args):
        """
        set_support_vectors(self, svs)

        set support vectors to given values

        Parameters:
        -----------

        svs:  array with all support vectors to set

        d:  number of support vectors 
        """
        return _Classifier.CKernelMachine_set_support_vectors(*args)

    def get_support_vectors(*args):
        """
        get_support_vectors(self) -> [] of int

        get all support vectors (swig compatible)

        Parameters:
        -----------

        svs:  array to contain a copy of the support vectors

        num:  number of support vectors in the array 
        """
        return _Classifier.CKernelMachine_get_support_vectors(*args)

    def get_alphas(*args):
        """
        get_alphas(self) -> [] of float

        get all alphas (swig compatible)

        Parameters:
        -----------

        alphas:  array to contain a copy of the alphas

        d1:  number of alphas in the array 
        """
        return _Classifier.CKernelMachine_get_alphas(*args)

    def create_new_model(*args):
        """
        create_new_model(self, num) -> bool

        create new model

        Parameters:
        -----------

        num:  number of alphas and support vectors in new model 
        """
        return _Classifier.CKernelMachine_create_new_model(*args)

    def init_kernel_optimization(*args):
        """
        init_kernel_optimization(self) -> bool

        initialise kernel optimisation

        if operation was successful 
        """
        return _Classifier.CKernelMachine_init_kernel_optimization(*args)

    def classify(*args):
        """
        classify(self) -> CLabels
        classify(self, data) -> CLabels

        classify objects

        Parameters:
        -----------

        data:  (test) data to be classified

        classified labels 
        """
        return _Classifier.CKernelMachine_classify(*args)

    def classify_example_helper(*args):
        """classify_example_helper(p) -> void"""
        return _Classifier.CKernelMachine_classify_example_helper(*args)

    if _newclass:classify_example_helper = staticmethod(classify_example_helper)
    __swig_getmethods__["classify_example_helper"] = lambda x: classify_example_helper
CKernelMachine_swigregister = _Classifier.CKernelMachine_swigregister
CKernelMachine_swigregister(CKernelMachine)

def CKernelMachine_classify_example_helper(*args):
  """CKernelMachine_classify_example_helper(p) -> void"""
  return _Classifier.CKernelMachine_classify_example_helper(*args)

class CDistanceMachine(Classifier):
    """
    A generic DistanceMachine interface.

    A distance machine is based on a a-priori choosen distance.

    C++ includes: DistanceMachine.h 
    """
    __swig_setmethods__ = {}
    for _s in [Classifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, CDistanceMachine, name, value)
    __swig_getmethods__ = {}
    for _s in [Classifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, CDistanceMachine, name)
    def __init__(self, *args, **kwargs): raise AttributeError, "No constructor defined"
    __repr__ = _swig_repr
    __swig_destroy__ = _Classifier.delete_CDistanceMachine
    __del__ = lambda self : None;
    def set_distance(*args):
        """
        set_distance(self, d)

        set distance

        Parameters:
        -----------

        d:  distance to set 
        """
        return _Classifier.CDistanceMachine_set_distance(*args)

    def get_distance(*args):
        """
        get_distance(self) -> CDistance

        get distance

        distance 
        """
        return _Classifier.CDistanceMachine_get_distance(*args)

    def distances_lhs(*args):
        """
        distances_lhs(self, result, idx_a1, idx_a2, idx_b)

        get distance functions for lhs feature vectors going from a1 to a2 and
        rhs feature vector b

        Parameters:
        -----------

        result:  array of distance values

        idx_a1:  first feature vector a1 at idx_a1

        idx_a2:  last feature vector a2 at idx_a2

        idx_b:  feature vector b at idx_b 
        """
        return _Classifier.CDistanceMachine_distances_lhs(*args)

    def distances_rhs(*args):
        """
        distances_rhs(self, result, idx_b1, idx_b2, idx_a)

        get distance functions for rhs feature vectors going from b1 to b2 and
        lhs feature vector a

        Parameters:
        -----------

        result:  array of distance values

        idx_b1:  first feature vector a1 at idx_b1

        idx_b2:  last feature vector a2 at idx_b2

        idx_a:  feature vector a at idx_a 
        """
        return _Classifier.CDistanceMachine_distances_rhs(*args)

CDistanceMachine_swigregister = _Classifier.CDistanceMachine_swigregister
CDistanceMachine_swigregister(CDistanceMachine)

class CSVM(CKernelMachine):
    """
    A generic Support Vector Machine Interface.

    A support vector machine is defined as \\[ f({\\bf
    x})=\\sum_{i=0}^{N-1} \\alpha_i k({\\bf x}, {\\bf x_i})+b
    \\]

    where $N$ is the number of training examples $\\alpha_i$ are the
    weights assigned to each training example $k(x,x')$ is the kernel and
    $b$ the bias.

    Using an a-priori choosen kernel, the $\\alpha_i$ and bias are
    determined by solving the following quadratic program

    \\begin{eqnarray*} \\max_{\\bf \\alpha} && \\sum_{i=0}^{N-1}
    \\alpha_i - \\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1} \\alpha_i y_i
    \\alpha_j y_j k({\\bf x_i}, {\\bf x_j})\\\\ \\mbox{s.t.}
    && 0\\leq\\alpha_i\\leq C\\\\ && \\sum_{i=0}^{N-1}
    \\alpha_i y_i=0\\\\ \\end{eqnarray*} here C is a pre-specified
    regularization parameter.

    C++ includes: SVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [CKernelMachine]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, CSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [CKernelMachine]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, CSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, num_sv=0) -> CSVM
        __init__(self) -> CSVM
        __init__(self, C, k, lab) -> CSVM

        Create a Support Vector Machine Object from a trained SVM

        Parameters:
        -----------

        C:  the C parameter

        k:  the Kernel object

        lab:  the Label object 
        """
        this = _Classifier.new_CSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_CSVM
    __del__ = lambda self : None;
    def set_defaults(*args):
        """
        set_defaults(self, num_sv=0)
        set_defaults(self)

        set default values for members a SVM object 
        """
        return _Classifier.CSVM_set_defaults(*args)

    def get_linear_term(*args):
        """
        get_linear_term(self) -> DoubleVector

        get linear term

        lin the linear term 
        """
        return _Classifier.CSVM_get_linear_term(*args)

    def set_linear_term(*args):
        """
        set_linear_term(self, lin)

        set linear term of the QP

        Parameters:
        -----------

        lin:  the linear term 
        """
        return _Classifier.CSVM_set_linear_term(*args)

    def set_nu(*args):
        """
        set_nu(self, nue)

        set nu

        Parameters:
        -----------

        nue:  new nu 
        """
        return _Classifier.CSVM_set_nu(*args)

    def set_C(*args):
        """
        set_C(self, c1, c2)

        set C

        Parameters:
        -----------

        c1:  new C constant for negatively labelled examples

        c2:  new C constant for positively labelled examples

        Note that not all SVMs support this (however at least CLibSVM and
        CSVMLight do) 
        """
        return _Classifier.CSVM_set_C(*args)

    def set_epsilon(*args):
        """
        set_epsilon(self, eps)

        set epsilon

        Parameters:
        -----------

        eps:  new epsilon 
        """
        return _Classifier.CSVM_set_epsilon(*args)

    def set_tube_epsilon(*args):
        """
        set_tube_epsilon(self, eps)

        set tube epsilon

        Parameters:
        -----------

        eps:  new tube epsilon 
        """
        return _Classifier.CSVM_set_tube_epsilon(*args)

    def set_qpsize(*args):
        """
        set_qpsize(self, qps)

        set qpsize

        Parameters:
        -----------

        qps:  new qpsize 
        """
        return _Classifier.CSVM_set_qpsize(*args)

    def get_epsilon(*args):
        """
        get_epsilon(self) -> float

        get epsilon

        epsilon 
        """
        return _Classifier.CSVM_get_epsilon(*args)

    def get_nu(*args):
        """
        get_nu(self) -> float

        get nu

        nu 
        """
        return _Classifier.CSVM_get_nu(*args)

    def get_C1(*args):
        """
        get_C1(self) -> float

        get C1

        C1 
        """
        return _Classifier.CSVM_get_C1(*args)

    def get_C2(*args):
        """
        get_C2(self) -> float

        get C2

        C2 
        """
        return _Classifier.CSVM_get_C2(*args)

    def get_qpsize(*args):
        """
        get_qpsize(self) ->  int

        get qpsize

        qpsize 
        """
        return _Classifier.CSVM_get_qpsize(*args)

    def set_shrinking_enabled(*args):
        """
        set_shrinking_enabled(self, enable)

        set state of shrinking

        Parameters:
        -----------

        enable:  if shrinking will be enabled 
        """
        return _Classifier.CSVM_set_shrinking_enabled(*args)

    def get_shrinking_enabled(*args):
        """
        get_shrinking_enabled(self) -> bool

        get state of shrinking

        if shrinking is enabled 
        """
        return _Classifier.CSVM_get_shrinking_enabled(*args)

    def compute_svm_dual_objective(*args):
        """
        compute_svm_dual_objective(self) -> float

        compute svm dual objective

        computed dual objective 
        """
        return _Classifier.CSVM_compute_svm_dual_objective(*args)

    def compute_svm_primal_objective(*args):
        """
        compute_svm_primal_objective(self) -> float

        compute svm primal objective

        computed svm primal objective 
        """
        return _Classifier.CSVM_compute_svm_primal_objective(*args)

    def set_objective(*args):
        """
        set_objective(self, v)

        set objective

        Parameters:
        -----------

        v:  objective 
        """
        return _Classifier.CSVM_set_objective(*args)

    def get_objective(*args):
        """
        get_objective(self) -> float

        get objective

        objective 
        """
        return _Classifier.CSVM_get_objective(*args)

    def set_callback_function(*args):
        """
        set_callback_function(self, m, cb)

        set callback function svm optimizers may call when they have a new
        (small) set of alphas

        Parameters:
        -----------

        m:  pointer to mkl object

        cb:  callback function 
        """
        return _Classifier.CSVM_set_callback_function(*args)

CSVM_swigregister = _Classifier.CSVM_swigregister
CSVM_swigregister(CSVM)

ONE_VS_REST = _Classifier.ONE_VS_REST
ONE_VS_ONE = _Classifier.ONE_VS_ONE
class MultiClassSVM(CSVM):
    """
    class MultiClassSVM

    C++ includes: MultiClassSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [CSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, MultiClassSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [CSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, MultiClassSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, type) -> MultiClassSVM
        __init__(self, type, C, k, lab) -> MultiClassSVM

        constructor

        Parameters:
        -----------

        type:  type of MultiClassSVM

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_MultiClassSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_MultiClassSVM
    __del__ = lambda self : None;
    def create_multiclass_svm(*args):
        """
        create_multiclass_svm(self, num_classes) -> bool

        create multiclass SVM

        Parameters:
        -----------

        num_classes:  number of classes in SVM

        if creation was successful 
        """
        return _Classifier.MultiClassSVM_create_multiclass_svm(*args)

    def set_svm(*args):
        """
        set_svm(self, num, svm) -> bool

        set SVM

        Parameters:
        -----------

        num:  number to set

        svm:  SVM to set

        if setting was successful 
        """
        return _Classifier.MultiClassSVM_set_svm(*args)

    def get_svm(*args):
        """
        get_svm(self, num) -> CSVM

        get SVM

        Parameters:
        -----------

        num:  which SVM to get

        SVM at number num 
        """
        return _Classifier.MultiClassSVM_get_svm(*args)

    def get_num_svms(*args):
        """
        get_num_svms(self) ->  int

        get number of SVMs

        number of SVMs 
        """
        return _Classifier.MultiClassSVM_get_num_svms(*args)

    def cleanup(*args):
        """
        cleanup(self)

        cleanup SVM 
        """
        return _Classifier.MultiClassSVM_cleanup(*args)

    def classify_one_vs_rest(*args):
        """
        classify_one_vs_rest(self) -> CLabels

        classify one vs rest

        resulting labels 
        """
        return _Classifier.MultiClassSVM_classify_one_vs_rest(*args)

    def classify_example_one_vs_rest(*args):
        """
        classify_example_one_vs_rest(self, num) -> float

        classify one example one vs rest

        Parameters:
        -----------

        num:  number of example of classify

        resulting classification 
        """
        return _Classifier.MultiClassSVM_classify_example_one_vs_rest(*args)

    def classify_one_vs_one(*args):
        """
        classify_one_vs_one(self) -> CLabels

        classify one vs one

        resulting labels 
        """
        return _Classifier.MultiClassSVM_classify_one_vs_one(*args)

    def classify_example_one_vs_one(*args):
        """
        classify_example_one_vs_one(self, num) -> float

        classify one example one vs one

        Parameters:
        -----------

        num:  number of example of classify

        resulting classification 
        """
        return _Classifier.MultiClassSVM_classify_example_one_vs_one(*args)

MultiClassSVM_swigregister = _Classifier.MultiClassSVM_swigregister
MultiClassSVM_swigregister(MultiClassSVM)

class LinearClassifier(Classifier):
    """
    Class LinearClassifier is a generic interface for all kinds of linear
    classifiers.

    A linear classifier computes

    \\[ f({\\bf x})= {\\bf w} \\cdot {\\bf x} + b \\]

    where ${\\bf w}$ are the weights assigned to each feature in
    training and $b$ the bias.

    To implement a linear classifier all that is required is to define the
    train() function that delivers ${\\bf w}$ above.

    Note that this framework works with linear classifiers of arbitraty
    feature type, e.g. dense and sparse and even string based features.
    This is implemented by using CDotFeatures that may provide a mapping
    function $\\Phi({\\bf x})\\mapsto {\\cal R^D}$ encapsulating
    all the required operations (like the dot product). The decision
    function is thus

    \\[ f({\\bf x})= {\\bf w} \\cdot \\Phi({\\bf x}) + b.
    \\]

    The following linear classifiers are implemented Linear Descriminant
    Analysis (CLDA)

    Linear Programming Machines (CLPM, CLPBoost)

    Perceptron ( CPerceptron)

    Linear SVMs ( CSVMSGD, CLibLinear, CSVMOcas, CSVMLin, CSubgradientSVM)

    See:  CDotFeatures

    C++ includes: LinearClassifier.h 
    """
    __swig_setmethods__ = {}
    for _s in [Classifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, LinearClassifier, name, value)
    __swig_getmethods__ = {}
    for _s in [Classifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, LinearClassifier, name)
    def __init__(self, *args, **kwargs): raise AttributeError, "No constructor defined"
    __repr__ = _swig_repr
    __swig_destroy__ = _Classifier.delete_LinearClassifier
    __del__ = lambda self : None;
    def get_w(*args):
        """
        get_w(self) -> [] of float
        get_w(self) -> [] of float

        get w (swig compatible)

        Parameters:
        -----------

        dst_w:  store w in this argument

        dst_dims:  dimension of w 
        """
        return _Classifier.LinearClassifier_get_w(*args)

    def set_w(*args):
        """
        set_w(self, src_w)

        set w

        Parameters:
        -----------

        src_w:  new w

        src_w_dim:  dimension of new w 
        """
        return _Classifier.LinearClassifier_set_w(*args)

    def set_bias(*args):
        """
        set_bias(self, b)

        set bias

        Parameters:
        -----------

        b:  new bias 
        """
        return _Classifier.LinearClassifier_set_bias(*args)

    def get_bias(*args):
        """
        get_bias(self) -> float

        get bias

        bias 
        """
        return _Classifier.LinearClassifier_get_bias(*args)

    def set_features(*args):
        """
        set_features(self, feat)

        set features

        Parameters:
        -----------

        feat:  features to set 
        """
        return _Classifier.LinearClassifier_set_features(*args)

    def classify(*args):
        """
        classify(self) -> CLabels
        classify(self, data) -> CLabels

        classify objects

        Parameters:
        -----------

        data:  (test) data to be classified

        classified labels 
        """
        return _Classifier.LinearClassifier_classify(*args)

    def get_features(*args):
        """
        get_features(self) -> CDotFeatures

        get features

        features 
        """
        return _Classifier.LinearClassifier_get_features(*args)

LinearClassifier_swigregister = _Classifier.LinearClassifier_swigregister
LinearClassifier_swigregister(LinearClassifier)

class GMNPSVM(MultiClassSVM):
    """
    Class GMNPSVM implements a one vs. rest MultiClass SVM.

    It uses CGMNPLib for training (in true multiclass-SVM fashion).

    C++ includes: GMNPSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [MultiClassSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, GMNPSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [MultiClassSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, GMNPSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> GMNPSVM
        __init__(self, C, k, lab) -> GMNPSVM

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_GMNPSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_GMNPSVM
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.GMNPSVM_train(*args)

    def getbasealphas(*args):
        """
        getbasealphas(self, basealphas)

        required for CMKLMulticlass constraint computation

        Parameters:
        -----------

        basealphas:  basealphas[k][j] is the alpha for class k and sample j
        which is untransformed compared to the alphas stored in CSVM* members

        """
        return _Classifier.GMNPSVM_getbasealphas(*args)

GMNPSVM_swigregister = _Classifier.GMNPSVM_swigregister
GMNPSVM_swigregister(GMNPSVM)

class GNPPSVM(CSVM):
    """
    class GNPPSVM

    C++ includes: GNPPSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [CSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, GNPPSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [CSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, GNPPSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> GNPPSVM
        __init__(self, C, k, lab) -> GNPPSVM

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_GNPPSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_GNPPSVM
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.GNPPSVM_train(*args)

GNPPSVM_swigregister = _Classifier.GNPPSVM_swigregister
GNPPSVM_swigregister(GNPPSVM)

class GPBTSVM(CSVM):
    """
    class GPBTSVM

    C++ includes: GPBTSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [CSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, GPBTSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [CSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, GPBTSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> GPBTSVM
        __init__(self, C, k, lab) -> GPBTSVM

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_GPBTSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_GPBTSVM
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.GPBTSVM_train(*args)

GPBTSVM_swigregister = _Classifier.GPBTSVM_swigregister
GPBTSVM_swigregister(GPBTSVM)

class KernelPerceptron(CKernelMachine):
    """
    Class KernelPerceptron - currently unfinished implementation of a
    Kernel Perceptron.

    C++ includes: KernelPerceptron.h 
    """
    __swig_setmethods__ = {}
    for _s in [CKernelMachine]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, KernelPerceptron, name, value)
    __swig_getmethods__ = {}
    for _s in [CKernelMachine]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, KernelPerceptron, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> KernelPerceptron

        constructor 
        """
        this = _Classifier.new_KernelPerceptron(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_KernelPerceptron
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train kernel perceptron classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.KernelPerceptron_train(*args)

KernelPerceptron_swigregister = _Classifier.KernelPerceptron_swigregister
KernelPerceptron_swigregister(KernelPerceptron)

class KNN(CDistanceMachine):
    """
    Class KNN, an implementation of the standard k-nearest neigbor
    classifier.

    An example is classified to belong to the class of which the majority
    of the k closest examples belong to.

    To avoid ties, k should be an odd number. To define how close examples
    are k-NN requires a CDistance object to work with (e.g.,
    CEuclideanDistance ).

    Note that k-NN has zero training time but classification times
    increase dramatically with the number of examples. Also note that k-NN
    is capable of multi-class-classification.

    C++ includes: KNN.h 
    """
    __swig_setmethods__ = {}
    for _s in [CDistanceMachine]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, KNN, name, value)
    __swig_getmethods__ = {}
    for _s in [CDistanceMachine]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, KNN, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> KNN
        __init__(self, k, d, trainlab) -> KNN

        constructor

        Parameters:
        -----------

        k:  k

        d:  distance

        trainlab:  labels for training 
        """
        this = _Classifier.new_KNN(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_KNN
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train k-NN classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.KNN_train(*args)

    def classify(*args):
        """
        classify(self) -> CLabels
        classify(self, data) -> CLabels

        classify objects

        Parameters:
        -----------

        data:  (test) data to be classified

        classified labels 
        """
        return _Classifier.KNN_classify(*args)

    def classify_for_multiple_k(*args):
        """
        classify_for_multiple_k(self, output)

        classify all examples for 1...k

        Parameters:
        -----------

        output:  resulting labels for all k

        k_out:  number of columns (k)

        num_vec:  number of outputs 
        """
        return _Classifier.KNN_classify_for_multiple_k(*args)

    def set_k(*args):
        """
        set_k(self, p_k)

        set k

        Parameters:
        -----------

        p_k:  new k 
        """
        return _Classifier.KNN_set_k(*args)

    def get_k(*args):
        """
        get_k(self) ->  int

        get k

        k 
        """
        return _Classifier.KNN_get_k(*args)

KNN_swigregister = _Classifier.KNN_swigregister
KNN_swigregister(KNN)

class LDA(LinearClassifier):
    """Proxy of C++ LDA class"""
    __swig_setmethods__ = {}
    for _s in [LinearClassifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, LDA, name, value)
    __swig_getmethods__ = {}
    for _s in [LinearClassifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, LDA, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, gamma=0) -> LDA
        __init__(self) -> LDA
        __init__(self, gamma, traindat, trainlab) -> LDA
        """
        this = _Classifier.new_LDA(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_LDA
    __del__ = lambda self : None;
    def set_gamma(*args):
        """set_gamma(self, gamma)"""
        return _Classifier.LDA_set_gamma(*args)

    def get_gamma(*args):
        """get_gamma(self) -> float"""
        return _Classifier.LDA_get_gamma(*args)

    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.LDA_train(*args)

LDA_swigregister = _Classifier.LDA_swigregister
LDA_swigregister(LDA)

LR = _Classifier.LR
L2 = _Classifier.L2
L1 = _Classifier.L1
class LibLinear(LinearClassifier):
    """Proxy of C++ LibLinear class"""
    __swig_setmethods__ = {}
    for _s in [LinearClassifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, LibLinear, name, value)
    __swig_getmethods__ = {}
    for _s in [LinearClassifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, LibLinear, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, loss) -> LibLinear
        __init__(self, C, traindat, trainlab) -> LibLinear
        """
        this = _Classifier.new_LibLinear(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_LibLinear
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.LibLinear_train(*args)

    def set_C(*args):
        """set_C(self, c1, c2)"""
        return _Classifier.LibLinear_set_C(*args)

    def get_C1(*args):
        """get_C1(self) -> float"""
        return _Classifier.LibLinear_get_C1(*args)

    def get_C2(*args):
        """get_C2(self) -> float"""
        return _Classifier.LibLinear_get_C2(*args)

    def set_epsilon(*args):
        """set_epsilon(self, eps)"""
        return _Classifier.LibLinear_set_epsilon(*args)

    def get_epsilon(*args):
        """get_epsilon(self) -> float"""
        return _Classifier.LibLinear_get_epsilon(*args)

    def set_bias_enabled(*args):
        """set_bias_enabled(self, enable_bias)"""
        return _Classifier.LibLinear_set_bias_enabled(*args)

    def get_bias_enabled(*args):
        """get_bias_enabled(self) -> bool"""
        return _Classifier.LibLinear_get_bias_enabled(*args)

LibLinear_swigregister = _Classifier.LibLinear_swigregister
LibLinear_swigregister(LibLinear)

class ScatterSVM(MultiClassSVM):
    """
    ScatterSVM - Multiclass SVM.

    The ScatterSVM is an unpublished experimental true multiclass SVM.
    Details are availabe in the following technical report.

    Robert Jenssen and Marius Kloft and Alexander Zien and S\\"oren
    Sonnenburg and            Klaus-Robert M\\"{u}ller, A Multi-Class
    Support Vector Machine Based on Scatter Criteria, TR 014-2009 TU
    Berlin, 2009

    C++ includes: ScatterSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [MultiClassSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, ScatterSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [MultiClassSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, ScatterSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> ScatterSVM
        __init__(self, C, k, lab) -> ScatterSVM

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_ScatterSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_ScatterSVM
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.ScatterSVM_train(*args)

ScatterSVM_swigregister = _Classifier.ScatterSVM_swigregister
ScatterSVM_swigregister(ScatterSVM)

LIBSVM_C_SVC = _Classifier.LIBSVM_C_SVC
LIBSVM_NU_SVC = _Classifier.LIBSVM_NU_SVC
class LibSVM(CSVM):
    """
    LibSVM.

    C++ includes: LibSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [CSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, LibSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [CSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, LibSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, st=LIBSVM_C_SVC) -> LibSVM
        __init__(self) -> LibSVM
        __init__(self, C, k, lab) -> LibSVM

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_LibSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_LibSVM
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.LibSVM_train(*args)

LibSVM_swigregister = _Classifier.LibSVM_swigregister
LibSVM_swigregister(LibSVM)

class larank_kcache_s(_object):
    """Proxy of C++ larank_kcache_s class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, larank_kcache_s, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, larank_kcache_s, name)
    __repr__ = _swig_repr
    __swig_setmethods__["func"] = _Classifier.larank_kcache_s_func_set
    __swig_getmethods__["func"] = _Classifier.larank_kcache_s_func_get
    if _newclass:func = _swig_property(_Classifier.larank_kcache_s_func_get, _Classifier.larank_kcache_s_func_set)
    __swig_setmethods__["prevbuddy"] = _Classifier.larank_kcache_s_prevbuddy_set
    __swig_getmethods__["prevbuddy"] = _Classifier.larank_kcache_s_prevbuddy_get
    if _newclass:prevbuddy = _swig_property(_Classifier.larank_kcache_s_prevbuddy_get, _Classifier.larank_kcache_s_prevbuddy_set)
    __swig_setmethods__["nextbuddy"] = _Classifier.larank_kcache_s_nextbuddy_set
    __swig_getmethods__["nextbuddy"] = _Classifier.larank_kcache_s_nextbuddy_get
    if _newclass:nextbuddy = _swig_property(_Classifier.larank_kcache_s_nextbuddy_get, _Classifier.larank_kcache_s_nextbuddy_set)
    __swig_setmethods__["maxsize"] = _Classifier.larank_kcache_s_maxsize_set
    __swig_getmethods__["maxsize"] = _Classifier.larank_kcache_s_maxsize_get
    if _newclass:maxsize = _swig_property(_Classifier.larank_kcache_s_maxsize_get, _Classifier.larank_kcache_s_maxsize_set)
    __swig_setmethods__["cursize"] = _Classifier.larank_kcache_s_cursize_set
    __swig_getmethods__["cursize"] = _Classifier.larank_kcache_s_cursize_get
    if _newclass:cursize = _swig_property(_Classifier.larank_kcache_s_cursize_get, _Classifier.larank_kcache_s_cursize_set)
    __swig_setmethods__["l"] = _Classifier.larank_kcache_s_l_set
    __swig_getmethods__["l"] = _Classifier.larank_kcache_s_l_get
    if _newclass:l = _swig_property(_Classifier.larank_kcache_s_l_get, _Classifier.larank_kcache_s_l_set)
    __swig_setmethods__["i2r"] = _Classifier.larank_kcache_s_i2r_set
    __swig_getmethods__["i2r"] = _Classifier.larank_kcache_s_i2r_get
    if _newclass:i2r = _swig_property(_Classifier.larank_kcache_s_i2r_get, _Classifier.larank_kcache_s_i2r_set)
    __swig_setmethods__["r2i"] = _Classifier.larank_kcache_s_r2i_set
    __swig_getmethods__["r2i"] = _Classifier.larank_kcache_s_r2i_get
    if _newclass:r2i = _swig_property(_Classifier.larank_kcache_s_r2i_get, _Classifier.larank_kcache_s_r2i_set)
    __swig_setmethods__["maxrowlen"] = _Classifier.larank_kcache_s_maxrowlen_set
    __swig_getmethods__["maxrowlen"] = _Classifier.larank_kcache_s_maxrowlen_get
    if _newclass:maxrowlen = _swig_property(_Classifier.larank_kcache_s_maxrowlen_get, _Classifier.larank_kcache_s_maxrowlen_set)
    __swig_setmethods__["rsize"] = _Classifier.larank_kcache_s_rsize_set
    __swig_getmethods__["rsize"] = _Classifier.larank_kcache_s_rsize_get
    if _newclass:rsize = _swig_property(_Classifier.larank_kcache_s_rsize_get, _Classifier.larank_kcache_s_rsize_set)
    __swig_setmethods__["rdiag"] = _Classifier.larank_kcache_s_rdiag_set
    __swig_getmethods__["rdiag"] = _Classifier.larank_kcache_s_rdiag_get
    if _newclass:rdiag = _swig_property(_Classifier.larank_kcache_s_rdiag_get, _Classifier.larank_kcache_s_rdiag_set)
    __swig_setmethods__["rdata"] = _Classifier.larank_kcache_s_rdata_set
    __swig_getmethods__["rdata"] = _Classifier.larank_kcache_s_rdata_get
    if _newclass:rdata = _swig_property(_Classifier.larank_kcache_s_rdata_get, _Classifier.larank_kcache_s_rdata_set)
    __swig_setmethods__["rnext"] = _Classifier.larank_kcache_s_rnext_set
    __swig_getmethods__["rnext"] = _Classifier.larank_kcache_s_rnext_get
    if _newclass:rnext = _swig_property(_Classifier.larank_kcache_s_rnext_get, _Classifier.larank_kcache_s_rnext_set)
    __swig_setmethods__["rprev"] = _Classifier.larank_kcache_s_rprev_set
    __swig_getmethods__["rprev"] = _Classifier.larank_kcache_s_rprev_get
    if _newclass:rprev = _swig_property(_Classifier.larank_kcache_s_rprev_get, _Classifier.larank_kcache_s_rprev_set)
    __swig_setmethods__["qnext"] = _Classifier.larank_kcache_s_qnext_set
    __swig_getmethods__["qnext"] = _Classifier.larank_kcache_s_qnext_get
    if _newclass:qnext = _swig_property(_Classifier.larank_kcache_s_qnext_get, _Classifier.larank_kcache_s_qnext_set)
    __swig_setmethods__["qprev"] = _Classifier.larank_kcache_s_qprev_set
    __swig_getmethods__["qprev"] = _Classifier.larank_kcache_s_qprev_get
    if _newclass:qprev = _swig_property(_Classifier.larank_kcache_s_qprev_get, _Classifier.larank_kcache_s_qprev_set)
    def __init__(self, *args): 
        """__init__(self) -> larank_kcache_s"""
        this = _Classifier.new_larank_kcache_s(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_larank_kcache_s
    __del__ = lambda self : None;
larank_kcache_s_swigregister = _Classifier.larank_kcache_s_swigregister
larank_kcache_s_swigregister(larank_kcache_s)

class LaRankOutput(_object):
    """Proxy of C++ LaRankOutput class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, LaRankOutput, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, LaRankOutput, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """__init__(self) -> LaRankOutput"""
        this = _Classifier.new_LaRankOutput(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_LaRankOutput
    __del__ = lambda self : None;
    def initialize(*args):
        """initialize(self, kfunc, cache)"""
        return _Classifier.LaRankOutput_initialize(*args)

    def destroy(*args):
        """destroy(self)"""
        return _Classifier.LaRankOutput_destroy(*args)

    def computeScore(*args):
        """computeScore(self, x_id) -> float"""
        return _Classifier.LaRankOutput_computeScore(*args)

    def computeGradient(*args):
        """computeGradient(self, xi_id, yi, ythis) -> float"""
        return _Classifier.LaRankOutput_computeGradient(*args)

    def update(*args):
        """update(self, x_id, lambda, gp)"""
        return _Classifier.LaRankOutput_update(*args)

    def set_kernel_buddy(*args):
        """set_kernel_buddy(self, bud)"""
        return _Classifier.LaRankOutput_set_kernel_buddy(*args)

    def cleanup(*args):
        """cleanup(self) ->  int"""
        return _Classifier.LaRankOutput_cleanup(*args)

    def getKernel(*args):
        """getKernel(self) -> larank_kcache_t"""
        return _Classifier.LaRankOutput_getKernel(*args)

    def get_l(*args):
        """get_l(self) ->  int"""
        return _Classifier.LaRankOutput_get_l(*args)

    def getW2(*args):
        """getW2(self) -> float"""
        return _Classifier.LaRankOutput_getW2(*args)

    def getKii(*args):
        """getKii(self, x_id) -> float"""
        return _Classifier.LaRankOutput_getKii(*args)

    def getBeta(*args):
        """getBeta(self, x_id) -> float"""
        return _Classifier.LaRankOutput_getBeta(*args)

    def getBetas(*args):
        """getBetas(self) -> float"""
        return _Classifier.LaRankOutput_getBetas(*args)

    def getGradient(*args):
        """getGradient(self, x_id) -> float"""
        return _Classifier.LaRankOutput_getGradient(*args)

    def isSupportVector(*args):
        """isSupportVector(self, x_id) -> bool"""
        return _Classifier.LaRankOutput_isSupportVector(*args)

    def getSV(*args):
        """getSV(self, sv) ->  int"""
        return _Classifier.LaRankOutput_getSV(*args)

LaRankOutput_swigregister = _Classifier.LaRankOutput_swigregister
LaRankOutput_swigregister(LaRankOutput)

class LaRankPattern(_object):
    """Proxy of C++ LaRankPattern class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, LaRankPattern, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, LaRankPattern, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, x_index, label) -> LaRankPattern
        __init__(self) -> LaRankPattern
        """
        this = _Classifier.new_LaRankPattern(*args)
        try: self.this.append(this)
        except: self.this = this
    def exists(*args):
        """exists(self) -> bool"""
        return _Classifier.LaRankPattern_exists(*args)

    def clear(*args):
        """clear(self)"""
        return _Classifier.LaRankPattern_clear(*args)

    __swig_setmethods__["x_id"] = _Classifier.LaRankPattern_x_id_set
    __swig_getmethods__["x_id"] = _Classifier.LaRankPattern_x_id_get
    if _newclass:x_id = _swig_property(_Classifier.LaRankPattern_x_id_get, _Classifier.LaRankPattern_x_id_set)
    __swig_setmethods__["y"] = _Classifier.LaRankPattern_y_set
    __swig_getmethods__["y"] = _Classifier.LaRankPattern_y_get
    if _newclass:y = _swig_property(_Classifier.LaRankPattern_y_get, _Classifier.LaRankPattern_y_set)
    __swig_destroy__ = _Classifier.delete_LaRankPattern
    __del__ = lambda self : None;
LaRankPattern_swigregister = _Classifier.LaRankPattern_swigregister
LaRankPattern_swigregister(LaRankPattern)

class LaRankPatterns(_object):
    """Proxy of C++ LaRankPatterns class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, LaRankPatterns, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, LaRankPatterns, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """__init__(self) -> LaRankPatterns"""
        this = _Classifier.new_LaRankPatterns(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_LaRankPatterns
    __del__ = lambda self : None;
    def insert(*args):
        """insert(self, pattern)"""
        return _Classifier.LaRankPatterns_insert(*args)

    def remove(*args):
        """remove(self, i)"""
        return _Classifier.LaRankPatterns_remove(*args)

    def empty(*args):
        """empty(self) -> bool"""
        return _Classifier.LaRankPatterns_empty(*args)

    def size(*args):
        """size(self) -> int"""
        return _Classifier.LaRankPatterns_size(*args)

    def sample(*args):
        """sample(self) -> LaRankPattern"""
        return _Classifier.LaRankPatterns_sample(*args)

    def getPatternRank(*args):
        """getPatternRank(self, x_id) -> int"""
        return _Classifier.LaRankPatterns_getPatternRank(*args)

    def isPattern(*args):
        """isPattern(self, x_id) -> bool"""
        return _Classifier.LaRankPatterns_isPattern(*args)

    def getPattern(*args):
        """getPattern(self, x_id) -> LaRankPattern"""
        return _Classifier.LaRankPatterns_getPattern(*args)

    def maxcount(*args):
        """maxcount(self) -> int"""
        return _Classifier.LaRankPatterns_maxcount(*args)

LaRankPatterns_swigregister = _Classifier.LaRankPatterns_swigregister
LaRankPatterns_swigregister(LaRankPatterns)

class LaRank(MultiClassSVM):
    """Proxy of C++ LaRank class"""
    __swig_setmethods__ = {}
    for _s in [MultiClassSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, LaRank, name, value)
    __swig_getmethods__ = {}
    for _s in [MultiClassSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, LaRank, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> LaRank
        __init__(self, C, k, lab) -> LaRank

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_LaRank(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_LaRank
    __del__ = lambda self : None;
    def add(*args):
        """add(self, x_id, yi) ->  int"""
        return _Classifier.LaRank_add(*args)

    def predict(*args):
        """predict(self, x_id) ->  int"""
        return _Classifier.LaRank_predict(*args)

    def destroy(*args):
        """destroy(self)"""
        return _Classifier.LaRank_destroy(*args)

    def computeGap(*args):
        """computeGap(self) -> float"""
        return _Classifier.LaRank_computeGap(*args)

    def getNumOutputs(*args):
        """getNumOutputs(self) -> int"""
        return _Classifier.LaRank_getNumOutputs(*args)

    def getNSV(*args):
        """getNSV(self) ->  int"""
        return _Classifier.LaRank_getNSV(*args)

    def computeW2(*args):
        """computeW2(self) -> float"""
        return _Classifier.LaRank_computeW2(*args)

    def getDual(*args):
        """getDual(self) -> float"""
        return _Classifier.LaRank_getDual(*args)

    def set_batch_mode(*args):
        """set_batch_mode(self, enable)"""
        return _Classifier.LaRank_set_batch_mode(*args)

    def get_batch_mode(*args):
        """get_batch_mode(self) -> bool"""
        return _Classifier.LaRank_get_batch_mode(*args)

    def set_tau(*args):
        """set_tau(self, t)"""
        return _Classifier.LaRank_set_tau(*args)

    def get_tau(*args):
        """get_tau(self) -> float"""
        return _Classifier.LaRank_get_tau(*args)

LaRank_swigregister = _Classifier.LaRank_swigregister
LaRank_swigregister(LaRank)

class LibSVMMultiClass(MultiClassSVM):
    """
    class LibSVMMultiClass

    C++ includes: LibSVMMultiClass.h 
    """
    __swig_setmethods__ = {}
    for _s in [MultiClassSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, LibSVMMultiClass, name, value)
    __swig_getmethods__ = {}
    for _s in [MultiClassSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, LibSVMMultiClass, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, st=LIBSVM_C_SVC) -> LibSVMMultiClass
        __init__(self) -> LibSVMMultiClass
        __init__(self, C, k, lab) -> LibSVMMultiClass

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_LibSVMMultiClass(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_LibSVMMultiClass
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train multiclass SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.LibSVMMultiClass_train(*args)

LibSVMMultiClass_swigregister = _Classifier.LibSVMMultiClass_swigregister
LibSVMMultiClass_swigregister(LibSVMMultiClass)

class LibSVMOneClass(CSVM):
    """
    class LibSVMOneClass

    C++ includes: LibSVMOneClass.h 
    """
    __swig_setmethods__ = {}
    for _s in [CSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, LibSVMOneClass, name, value)
    __swig_getmethods__ = {}
    for _s in [CSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, LibSVMOneClass, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> LibSVMOneClass
        __init__(self, C, k) -> LibSVMOneClass

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel 
        """
        this = _Classifier.new_LibSVMOneClass(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_LibSVMOneClass
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.LibSVMOneClass_train(*args)

LibSVMOneClass_swigregister = _Classifier.LibSVMOneClass_swigregister
LibSVMOneClass_swigregister(LibSVMOneClass)

class MPDSVM(CSVM):
    """
    class MPDSVM

    C++ includes: MPDSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [CSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, MPDSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [CSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, MPDSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> MPDSVM
        __init__(self, C, k, lab) -> MPDSVM

        constructor

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_MPDSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_MPDSVM
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.MPDSVM_train(*args)

MPDSVM_swigregister = _Classifier.MPDSVM_swigregister
MPDSVM_swigregister(MPDSVM)

class Perceptron(LinearClassifier):
    """
    Class Perceptron implements the standard linear (online) perceptron.

    Given a maximum number of iterations (the standard perceptron
    algorithm is not guaranteed to converge) and a fixed lerning rate, the
    result is a linear classifier.

    See:   CLinearClassifier

    http://en.wikipedia.org/wiki/Perceptron

    C++ includes: Perceptron.h 
    """
    __swig_setmethods__ = {}
    for _s in [LinearClassifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, Perceptron, name, value)
    __swig_getmethods__ = {}
    for _s in [LinearClassifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, Perceptron, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> Perceptron
        __init__(self, traindat, trainlab) -> Perceptron

        constructor

        Parameters:
        -----------

        traindat:  training features

        trainlab:  labels for training features 
        """
        this = _Classifier.new_Perceptron(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_Perceptron
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.Perceptron_train(*args)

    def set_learn_rate(*args):
        """
        set_learn_rate(self, r)

        set learn rate of gradient descent training algorithm 
        """
        return _Classifier.Perceptron_set_learn_rate(*args)

    def set_max_iter(*args):
        """
        set_max_iter(self, i)

        set maximum number of iterations 
        """
        return _Classifier.Perceptron_set_max_iter(*args)

Perceptron_swigregister = _Classifier.Perceptron_swigregister
Perceptron_swigregister(Perceptron)

class SubGradientSVM(LinearClassifier):
    """
    class SubGradientSVM

    C++ includes: SubGradientSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [LinearClassifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, SubGradientSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [LinearClassifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, SubGradientSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> SubGradientSVM
        __init__(self, C, traindat, trainlab) -> SubGradientSVM

        constructor

        Parameters:
        -----------

        C:  constant C

        traindat:  training features

        trainlab:  labels for training features 
        """
        this = _Classifier.new_SubGradientSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_SubGradientSVM
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.SubGradientSVM_train(*args)

    def set_C(*args):
        """
        set_C(self, c1, c2)

        set C

        Parameters:
        -----------

        c1:  new C1

        c2:  new C2 
        """
        return _Classifier.SubGradientSVM_set_C(*args)

    def get_C1(*args):
        """
        get_C1(self) -> float

        get C1

        C1 
        """
        return _Classifier.SubGradientSVM_get_C1(*args)

    def get_C2(*args):
        """
        get_C2(self) -> float

        get C2

        C2 
        """
        return _Classifier.SubGradientSVM_get_C2(*args)

    def set_bias_enabled(*args):
        """
        set_bias_enabled(self, enable_bias)

        set if bias shall be enabled

        Parameters:
        -----------

        enable_bias:  if bias shall be enabled 
        """
        return _Classifier.SubGradientSVM_set_bias_enabled(*args)

    def get_bias_enabled(*args):
        """
        get_bias_enabled(self) -> bool

        check if bias is enabled

        if bias is enabled 
        """
        return _Classifier.SubGradientSVM_get_bias_enabled(*args)

    def set_epsilon(*args):
        """
        set_epsilon(self, eps)

        set epsilon

        Parameters:
        -----------

        eps:  new epsilon 
        """
        return _Classifier.SubGradientSVM_set_epsilon(*args)

    def get_epsilon(*args):
        """
        get_epsilon(self) -> float

        get epsilon

        epsilon 
        """
        return _Classifier.SubGradientSVM_get_epsilon(*args)

    def set_qpsize(*args):
        """
        set_qpsize(self, q)

        set qpsize

        Parameters:
        -----------

        q:  new qpsize 
        """
        return _Classifier.SubGradientSVM_set_qpsize(*args)

    def get_qpsize(*args):
        """
        get_qpsize(self) ->  int

        get qpsize

        qpsize 
        """
        return _Classifier.SubGradientSVM_get_qpsize(*args)

    def set_qpsize_max(*args):
        """
        set_qpsize_max(self, q)

        set qpsize_max

        Parameters:
        -----------

        q:  new qpsize_max 
        """
        return _Classifier.SubGradientSVM_set_qpsize_max(*args)

    def get_qpsize_max(*args):
        """
        get_qpsize_max(self) ->  int

        get qpsize_max

        qpsize_max 
        """
        return _Classifier.SubGradientSVM_get_qpsize_max(*args)

SubGradientSVM_swigregister = _Classifier.SubGradientSVM_swigregister
SubGradientSVM_swigregister(SubGradientSVM)

class SVMLin(LinearClassifier):
    """
    class SVMLin

    C++ includes: SVMLin.h 
    """
    __swig_setmethods__ = {}
    for _s in [LinearClassifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, SVMLin, name, value)
    __swig_getmethods__ = {}
    for _s in [LinearClassifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, SVMLin, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> SVMLin
        __init__(self, C, traindat, trainlab) -> SVMLin

        constructor

        Parameters:
        -----------

        C:  constant C

        traindat:  training features

        trainlab:  labels for features 
        """
        this = _Classifier.new_SVMLin(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_SVMLin
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.SVMLin_train(*args)

    def set_C(*args):
        """
        set_C(self, c1, c2)

        set C

        Parameters:
        -----------

        c1:  new C1

        c2:  new C2 
        """
        return _Classifier.SVMLin_set_C(*args)

    def get_C1(*args):
        """
        get_C1(self) -> float

        get C1

        C1 
        """
        return _Classifier.SVMLin_get_C1(*args)

    def get_C2(*args):
        """
        get_C2(self) -> float

        get C2

        C2 
        """
        return _Classifier.SVMLin_get_C2(*args)

    def set_bias_enabled(*args):
        """
        set_bias_enabled(self, enable_bias)

        set if bias shall be enabled

        Parameters:
        -----------

        enable_bias:  if bias shall be enabled 
        """
        return _Classifier.SVMLin_set_bias_enabled(*args)

    def get_bias_enabled(*args):
        """
        get_bias_enabled(self) -> bool

        get if bias is enabled

        if bias is enabled 
        """
        return _Classifier.SVMLin_get_bias_enabled(*args)

    def set_epsilon(*args):
        """
        set_epsilon(self, eps)

        set epsilon

        Parameters:
        -----------

        eps:  new epsilon 
        """
        return _Classifier.SVMLin_set_epsilon(*args)

    def get_epsilon(*args):
        """
        get_epsilon(self) -> float

        get epsilon

        epsilon 
        """
        return _Classifier.SVMLin_get_epsilon(*args)

SVMLin_swigregister = _Classifier.SVMLin_swigregister
SVMLin_swigregister(SVMLin)

SVM_OCAS = _Classifier.SVM_OCAS
SVM_BMRM = _Classifier.SVM_BMRM
class SVMOcas(LinearClassifier):
    """
    class SVMOcas

    C++ includes: SVMOcas.h 
    """
    __swig_setmethods__ = {}
    for _s in [LinearClassifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, SVMOcas, name, value)
    __swig_getmethods__ = {}
    for _s in [LinearClassifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, SVMOcas, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, type) -> SVMOcas
        __init__(self, C, traindat, trainlab) -> SVMOcas

        constructor

        Parameters:
        -----------

        C:  constant C

        traindat:  training features

        trainlab:  labels for training features 
        """
        this = _Classifier.new_SVMOcas(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_SVMOcas
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.SVMOcas_train(*args)

    def set_C(*args):
        """
        set_C(self, c1, c2)

        set C

        Parameters:
        -----------

        c1:  new C1

        c2:  new C2 
        """
        return _Classifier.SVMOcas_set_C(*args)

    def get_C1(*args):
        """
        get_C1(self) -> float

        get C1

        C1 
        """
        return _Classifier.SVMOcas_get_C1(*args)

    def get_C2(*args):
        """
        get_C2(self) -> float

        get C2

        C2 
        """
        return _Classifier.SVMOcas_get_C2(*args)

    def set_epsilon(*args):
        """
        set_epsilon(self, eps)

        set epsilon

        Parameters:
        -----------

        eps:  new epsilon 
        """
        return _Classifier.SVMOcas_set_epsilon(*args)

    def get_epsilon(*args):
        """
        get_epsilon(self) -> float

        get epsilon

        epsilon 
        """
        return _Classifier.SVMOcas_get_epsilon(*args)

    def set_bias_enabled(*args):
        """
        set_bias_enabled(self, enable_bias)

        set if bias shall be enabled

        Parameters:
        -----------

        enable_bias:  if bias shall be enabled 
        """
        return _Classifier.SVMOcas_set_bias_enabled(*args)

    def get_bias_enabled(*args):
        """
        get_bias_enabled(self) -> bool

        check if bias is enabled

        if bias is enabled 
        """
        return _Classifier.SVMOcas_get_bias_enabled(*args)

    def set_bufsize(*args):
        """
        set_bufsize(self, sz)

        set buffer size

        Parameters:
        -----------

        sz:  buffer size 
        """
        return _Classifier.SVMOcas_set_bufsize(*args)

    def get_bufsize(*args):
        """
        get_bufsize(self) ->  int

        get buffer size

        buffer size 
        """
        return _Classifier.SVMOcas_get_bufsize(*args)

SVMOcas_swigregister = _Classifier.SVMOcas_swigregister
SVMOcas_swigregister(SVMOcas)

class SVMSGD(LinearClassifier):
    """
    class SVMSGD

    C++ includes: SVMSGD.h 
    """
    __swig_setmethods__ = {}
    for _s in [LinearClassifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, SVMSGD, name, value)
    __swig_getmethods__ = {}
    for _s in [LinearClassifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, SVMSGD, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, C) -> SVMSGD
        __init__(self, C, traindat, trainlab) -> SVMSGD

        constructor

        Parameters:
        -----------

        C:  constant C

        traindat:  training features

        trainlab:  labels for training features 
        """
        this = _Classifier.new_SVMSGD(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_SVMSGD
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.SVMSGD_train(*args)

    def set_C(*args):
        """
        set_C(self, c1, c2)

        set C

        Parameters:
        -----------

        c1:  new C1

        c2:  new C2 
        """
        return _Classifier.SVMSGD_set_C(*args)

    def get_C1(*args):
        """
        get_C1(self) -> float

        get C1

        C1 
        """
        return _Classifier.SVMSGD_get_C1(*args)

    def get_C2(*args):
        """
        get_C2(self) -> float

        get C2

        C2 
        """
        return _Classifier.SVMSGD_get_C2(*args)

    def set_epochs(*args):
        """
        set_epochs(self, e)

        set epochs

        Parameters:
        -----------

        e:  new number of training epochs 
        """
        return _Classifier.SVMSGD_set_epochs(*args)

    def get_epochs(*args):
        """
        get_epochs(self) ->  int

        get epochs

        the number of training epochs 
        """
        return _Classifier.SVMSGD_get_epochs(*args)

    def set_bias_enabled(*args):
        """
        set_bias_enabled(self, enable_bias)

        set if bias shall be enabled

        Parameters:
        -----------

        enable_bias:  if bias shall be enabled 
        """
        return _Classifier.SVMSGD_set_bias_enabled(*args)

    def get_bias_enabled(*args):
        """
        get_bias_enabled(self) -> bool

        check if bias is enabled

        if bias is enabled 
        """
        return _Classifier.SVMSGD_get_bias_enabled(*args)

    def set_regularized_bias_enabled(*args):
        """
        set_regularized_bias_enabled(self, enable_bias)

        set if regularized bias shall be enabled

        Parameters:
        -----------

        enable_bias:  if regularized bias shall be enabled 
        """
        return _Classifier.SVMSGD_set_regularized_bias_enabled(*args)

    def get_regularized_bias_enabled(*args):
        """
        get_regularized_bias_enabled(self) -> bool

        check if regularized bias is enabled

        if regularized bias is enabled 
        """
        return _Classifier.SVMSGD_get_regularized_bias_enabled(*args)

SVMSGD_swigregister = _Classifier.SVMSGD_swigregister
SVMSGD_swigregister(SVMSGD)

class WDSVMOcas(Classifier):
    """
    class WDSVMOcas

    C++ includes: WDSVMOcas.h 
    """
    __swig_setmethods__ = {}
    for _s in [Classifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, WDSVMOcas, name, value)
    __swig_getmethods__ = {}
    for _s in [Classifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, WDSVMOcas, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, type) -> WDSVMOcas
        __init__(self, C, d, from_d, traindat, trainlab) -> WDSVMOcas

        constructor

        Parameters:
        -----------

        C:  constant C

        d:  degree

        from_d:  from degree

        traindat:  training features

        trainlab:  labels for training features 
        """
        this = _Classifier.new_WDSVMOcas(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_WDSVMOcas
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.WDSVMOcas_train(*args)

    def set_C(*args):
        """
        set_C(self, c1, c2)

        set C

        Parameters:
        -----------

        c1:  new C1

        c2:  new C2 
        """
        return _Classifier.WDSVMOcas_set_C(*args)

    def get_C1(*args):
        """
        get_C1(self) -> float

        get C1

        C1 
        """
        return _Classifier.WDSVMOcas_get_C1(*args)

    def get_C2(*args):
        """
        get_C2(self) -> float

        get C2

        C2 
        """
        return _Classifier.WDSVMOcas_get_C2(*args)

    def set_epsilon(*args):
        """
        set_epsilon(self, eps)

        set epsilon

        Parameters:
        -----------

        eps:  new epsilon 
        """
        return _Classifier.WDSVMOcas_set_epsilon(*args)

    def get_epsilon(*args):
        """
        get_epsilon(self) -> float

        get epsilon

        epsilon 
        """
        return _Classifier.WDSVMOcas_get_epsilon(*args)

    def set_features(*args):
        """
        set_features(self, feat)

        set features

        Parameters:
        -----------

        feat:  features to set 
        """
        return _Classifier.WDSVMOcas_set_features(*args)

    def get_features(*args):
        """
        get_features(self) -> shogun::CStringFeatures<(uint8_t)>

        get features

        features 
        """
        return _Classifier.WDSVMOcas_get_features(*args)

    def set_bias_enabled(*args):
        """
        set_bias_enabled(self, enable_bias)

        set if bias shall be enabled

        Parameters:
        -----------

        enable_bias:  if bias shall be enabled 
        """
        return _Classifier.WDSVMOcas_set_bias_enabled(*args)

    def get_bias_enabled(*args):
        """
        get_bias_enabled(self) -> bool

        check if bias is enabled

        if bias is enabled 
        """
        return _Classifier.WDSVMOcas_get_bias_enabled(*args)

    def set_bufsize(*args):
        """
        set_bufsize(self, sz)

        set buffer size

        Parameters:
        -----------

        sz:  buffer size 
        """
        return _Classifier.WDSVMOcas_set_bufsize(*args)

    def get_bufsize(*args):
        """
        get_bufsize(self) ->  int

        get buffer size

        buffer size 
        """
        return _Classifier.WDSVMOcas_get_bufsize(*args)

    def set_degree(*args):
        """
        set_degree(self, d, from_d)

        set degree

        Parameters:
        -----------

        d:  degree

        from_d:  from degree 
        """
        return _Classifier.WDSVMOcas_set_degree(*args)

    def get_degree(*args):
        """
        get_degree(self) ->  int

        get degree

        degree 
        """
        return _Classifier.WDSVMOcas_get_degree(*args)

    def classify(*args):
        """
        classify(self) -> CLabels
        classify(self, data) -> CLabels

        classify objects

        Parameters:
        -----------

        data:  (test) data to be classified

        classified labels 
        """
        return _Classifier.WDSVMOcas_classify(*args)

    def set_normalization_const(*args):
        """
        set_normalization_const(self)

        set normalization const 
        """
        return _Classifier.WDSVMOcas_set_normalization_const(*args)

    def get_normalization_const(*args):
        """
        get_normalization_const(self) -> float

        get normalization const

        normalization const 
        """
        return _Classifier.WDSVMOcas_get_normalization_const(*args)

WDSVMOcas_swigregister = _Classifier.WDSVMOcas_swigregister
WDSVMOcas_swigregister(WDSVMOcas)

class PluginEstimate(Classifier):
    """
    class PluginEstimate

    The class PluginEstimate takes as input two probabilistic models (of
    type CLinearHMM, even though general models are possible ) and
    classifies examples according to the rule

    \\[ f({\\bf x})= \\log(\\mbox{Pr}({\\bf x}|\\theta_+)) -
    \\log(\\mbox{Pr}({\\bf x}|\\theta_-)) \\]

    See:  CLinearHMM

    CDistribution

    C++ includes: PluginEstimate.h 
    """
    __swig_setmethods__ = {}
    for _s in [Classifier]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, PluginEstimate, name, value)
    __swig_getmethods__ = {}
    for _s in [Classifier]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, PluginEstimate, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, pos_pseudo=1e-10, neg_pseudo=1e-10) -> PluginEstimate
        __init__(self, pos_pseudo=1e-10) -> PluginEstimate
        __init__(self) -> PluginEstimate

        default constructor

        Parameters:
        -----------

        pos_pseudo:  pseudo for positive model

        neg_pseudo:  pseudo for negative model 
        """
        this = _Classifier.new_PluginEstimate(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_PluginEstimate
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train plugin estimate classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.PluginEstimate_train(*args)

    def classify(*args):
        """
        classify(self) -> CLabels
        classify(self, data) -> CLabels

        classify objects

        Parameters:
        -----------

        data:  (test) data to be classified

        classified labels 
        """
        return _Classifier.PluginEstimate_classify(*args)

    def set_features(*args):
        """
        set_features(self, feat)

        set features

        Parameters:
        -----------

        feat:  features to set 
        """
        return _Classifier.PluginEstimate_set_features(*args)

    def get_features(*args):
        """
        get_features(self) -> shogun::CStringFeatures<(uint16_t)>

        get features

        features 
        """
        return _Classifier.PluginEstimate_get_features(*args)

    def posterior_log_odds_obsolete(*args):
        """
        posterior_log_odds_obsolete(self, vector, len) -> float

        obsolete posterior log odds

        Parameters:
        -----------

        vector:  vector

        len:  len

        something floaty 
        """
        return _Classifier.PluginEstimate_posterior_log_odds_obsolete(*args)

    def get_parameterwise_log_odds(*args):
        """
        get_parameterwise_log_odds(self, obs, position) -> float

        get log odds parameter-wise

        Parameters:
        -----------

        obs:  observation

        position:  position

        log odd at position 
        """
        return _Classifier.PluginEstimate_get_parameterwise_log_odds(*args)

    def log_derivative_pos_obsolete(*args):
        """
        log_derivative_pos_obsolete(self, obs, pos) -> float

        get obsolete positive log derivative

        Parameters:
        -----------

        obs:  observation

        pos:  position

        positive log derivative 
        """
        return _Classifier.PluginEstimate_log_derivative_pos_obsolete(*args)

    def log_derivative_neg_obsolete(*args):
        """
        log_derivative_neg_obsolete(self, obs, pos) -> float

        get obsolete negative log derivative

        Parameters:
        -----------

        obs:  observation

        pos:  position

        negative log derivative 
        """
        return _Classifier.PluginEstimate_log_derivative_neg_obsolete(*args)

    def get_model_params(*args):
        """
        get_model_params(self, pos_params, neg_params, seq_length, num_symbols) -> bool

        get model parameters

        Parameters:
        -----------

        pos_params:  parameters of positive model

        neg_params:  parameters of negative model

        seq_length:  sequence length

        num_symbols:  numbe of symbols

        if operation was successful 
        """
        return _Classifier.PluginEstimate_get_model_params(*args)

    def set_model_params(*args):
        """
        set_model_params(self, pos_params, neg_params, seq_length, num_symbols)

        set model parameters

        Parameters:
        -----------

        pos_params:  parameters of positive model

        neg_params:  parameters of negative model

        seq_length:  sequence length

        num_symbols:  numbe of symbols 
        """
        return _Classifier.PluginEstimate_set_model_params(*args)

    def get_num_params(*args):
        """
        get_num_params(self) ->  int

        get number of parameters

        number of parameters 
        """
        return _Classifier.PluginEstimate_get_num_params(*args)

    def check_models(*args):
        """
        check_models(self) -> bool

        check models

        if one of the two models is invalid 
        """
        return _Classifier.PluginEstimate_check_models(*args)

PluginEstimate_swigregister = _Classifier.PluginEstimate_swigregister
PluginEstimate_swigregister(PluginEstimate)

class MKL(CSVM):
    """
    Multiple Kernel Learning.

    A support vector machine based method for use with multiple kernels.
    In Multiple Kernel Learning (MKL) in addition to the SVM
    $\\bf\\alpha$ and bias term $b$ the kernel weights
    $\\bf\\beta$ are estimated in training. The resulting kernel
    method can be stated as

    \\[ f({\\bf x})=\\sum_{i=0}^{N-1} \\alpha_i \\sum_{j=0}^M
    \\beta_j k_j({\\bf x}, {\\bf x_i})+b . \\]

    where $N$ is the number of training examples $\\alpha_i$ are the
    weights assigned to each training example $\\beta_j$ are the weights
    assigned to each sub-kernel $k_j(x,x')$ are sub-kernels and $b$ the
    bias.

    Kernels have to be chosen a-priori. In MKL $\\alpha_i,\\;\\beta$
    and bias are determined by solving the following optimization program

    \\begin{eqnarray*} \\mbox{min} &&
    \\gamma-\\sum_{i=1}^N\\alpha_i\\\\ \\mbox{w.r.t.} &&
    \\gamma\\in R, \\alpha\\in R^N \\nonumber\\\\
    \\mbox{s.t.} && {\\bf 0}\\leq\\alpha\\leq{\\bf
    1}C,\\;\\;\\sum_{i=1}^N \\alpha_i y_i=0 \\nonumber\\\\
    && \\frac{1}{2}\\sum_{i,j=1}^N \\alpha_i \\alpha_j y_i y_j
    k_k({\\bf x}_i,{\\bf x}_j)\\leq \\gamma,\\;\\; \\forall
    k=1,\\ldots,K\\nonumber\\\\ \\end{eqnarray*} here C is a
    pre-specified regularization parameter.

    Within shogun this optimization problem is solved using semi-infinite
    programming. For 1-norm MKL using one of the two approaches described
    in

    Soeren Sonnenburg, Gunnar Raetsch, Christin Schaefer, and Bernhard
    Schoelkopf. Large Scale Multiple Kernel Learning. Journal of Machine
    Learning Research, 7:1531-1565, July 2006.

    The first approach (also called the wrapper algorithm) wrapps around a
    single kernel SVMs, alternatingly solving for $\\alpha$ and
    $\\beta$. It is using a traditional SVM to generate new violated
    constraings and thus requires a single kernel SVM and any of the SVMs
    contained in shogun can be used. In the MKL step either a linear
    program is solved via glpk or cplex or analytically or a newton (for
    norms>1) step is performed.

    The second much faster but also more memory demanding approach
    performing interleaved optimization, is integrated into the chunking-
    based SVMlight.

    In addition sparsity of MKL can be controlled by the choice of the
    $L_p$-norm regularizing $\\beta$ as described in

    Marius Kloft, Ulf Brefeld, Soeren Sonnenburg, and Alexander Zien.
    Efficient and accurate lp-norm multiple kernel learning. In Advances
    in Neural Information Processing Systems 21. MIT Press, Cambridge, MA,
    2009.

    C++ includes: MKL.h 
    """
    __swig_setmethods__ = {}
    for _s in [CSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, MKL, name, value)
    __swig_getmethods__ = {}
    for _s in [CSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, MKL, name)
    def __init__(self, *args, **kwargs): raise AttributeError, "No constructor defined"
    __repr__ = _swig_repr
    __swig_destroy__ = _Classifier.delete_MKL
    __del__ = lambda self : None;
    def set_constraint_generator(*args):
        """
        set_constraint_generator(self, s)

        SVM to use as constraint generator in MKL SIP

        Parameters:
        -----------

        s:  svm 
        """
        return _Classifier.MKL_set_constraint_generator(*args)

    def set_svm(*args):
        """
        set_svm(self, s)

        SVM to use as constraint generator in MKL SIP

        Parameters:
        -----------

        s:  svm 
        """
        return _Classifier.MKL_set_svm(*args)

    def get_svm(*args):
        """
        get_svm(self) -> CSVM

        get SVM that is used as constraint generator in MKL SIP

        svm 
        """
        return _Classifier.MKL_get_svm(*args)

    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train MKL classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.MKL_train(*args)

    def set_C_mkl(*args):
        """
        set_C_mkl(self, C)

        set C mkl

        Parameters:
        -----------

        C:  new C_mkl 
        """
        return _Classifier.MKL_set_C_mkl(*args)

    def set_mkl_norm(*args):
        """
        set_mkl_norm(self, norm)

        set mkl norm

        Parameters:
        -----------

        norm:  new mkl norm (must be greater equal 1) 
        """
        return _Classifier.MKL_set_mkl_norm(*args)

    def set_interleaved_optimization_enabled(*args):
        """
        set_interleaved_optimization_enabled(self, enable)

        set state of optimization (interleaved or wrapper)

        Parameters:
        -----------

        enable:  if true interleaved optimization is used; wrapper otherwise

        """
        return _Classifier.MKL_set_interleaved_optimization_enabled(*args)

    def get_interleaved_optimization_enabled(*args):
        """
        get_interleaved_optimization_enabled(self) -> bool

        get state of optimization (interleaved or wrapper)

        true if interleaved optimization is used; wrapper otherwise 
        """
        return _Classifier.MKL_get_interleaved_optimization_enabled(*args)

    def compute_mkl_primal_objective(*args):
        """
        compute_mkl_primal_objective(self) -> float

        compute mkl primal objective

        computed mkl primal objective 
        """
        return _Classifier.MKL_compute_mkl_primal_objective(*args)

    def compute_mkl_dual_objective(*args):
        """
        compute_mkl_dual_objective(self) -> float

        compute mkl dual objective

        computed dual objective 
        """
        return _Classifier.MKL_compute_mkl_dual_objective(*args)

    def set_mkl_epsilon(*args):
        """
        set_mkl_epsilon(self, eps)

        set mkl epsilon (optimization accuracy for kernel weights)

        Parameters:
        -----------

        eps:  new weight_epsilon 
        """
        return _Classifier.MKL_set_mkl_epsilon(*args)

    def get_mkl_epsilon(*args):
        """
        get_mkl_epsilon(self) -> float

        get mkl epsilon for weights (optimization accuracy for kernel weights)

        epsilon for weights 
        """
        return _Classifier.MKL_get_mkl_epsilon(*args)

    def get_mkl_iterations(*args):
        """
        get_mkl_iterations(self) ->  int

        get number of MKL iterations

        mkl_iterations 
        """
        return _Classifier.MKL_get_mkl_iterations(*args)

    def perform_mkl_step(*args):
        """
        perform_mkl_step(self, sumw, suma) -> bool

        perform single mkl iteration

        given sum of alphas, objectives for current alphas for each kernel and
        current kernel weighting compute the corresponding optimal kernel
        weighting (all via get/set_subkernel_weights in CCombinedKernel)

        Parameters:
        -----------

        sumw:  vector of 1/2*alpha'*K_j*alpha for each kernel j

        suma:  scalar sum_i alpha_i etc. 
        """
        return _Classifier.MKL_perform_mkl_step(*args)

    def perform_mkl_step_helper(*args):
        """perform_mkl_step_helper(mkl, sumw, suma) -> bool"""
        return _Classifier.MKL_perform_mkl_step_helper(*args)

    if _newclass:perform_mkl_step_helper = staticmethod(perform_mkl_step_helper)
    __swig_getmethods__["perform_mkl_step_helper"] = lambda x: perform_mkl_step_helper
    def compute_sum_alpha(*args):
        """
        compute_sum_alpha(self) -> float

        compute beta independent term from objective, e.g., in 2-class MKL
        sum_i alpha_i etc 
        """
        return _Classifier.MKL_compute_sum_alpha(*args)

    def compute_sum_beta(*args):
        """
        compute_sum_beta(self, sumw)

        compute 1/2*alpha'*K_j*alpha for each kernel j (beta dependent term
        from objective)

        Parameters:
        -----------

        sumw:  vector of size num_kernels to hold the result 
        """
        return _Classifier.MKL_compute_sum_beta(*args)

MKL_swigregister = _Classifier.MKL_swigregister
MKL_swigregister(MKL)

def MKL_perform_mkl_step_helper(*args):
  """MKL_perform_mkl_step_helper(mkl, sumw, suma) -> bool"""
  return _Classifier.MKL_perform_mkl_step_helper(*args)

class MKLClassification(MKL):
    """
    Multiple Kernel Learning for two-class-classification.

    Learns an SVM classifier and its kernel weights. Makes only sense if
    multiple kernels are used.

    See:   CMKL

    C++ includes: MKLClassification.h 
    """
    __swig_setmethods__ = {}
    for _s in [MKL]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, MKLClassification, name, value)
    __swig_getmethods__ = {}
    for _s in [MKL]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, MKLClassification, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, s=None) -> MKLClassification
        __init__(self) -> MKLClassification

        Constructor

        Parameters:
        -----------

        s:  SVM to use as constraint generator in MKL SILP 
        """
        this = _Classifier.new_MKLClassification(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_MKLClassification
    __del__ = lambda self : None;
MKLClassification_swigregister = _Classifier.MKLClassification_swigregister
MKLClassification_swigregister(MKLClassification)

class MKLOneClass(MKL):
    """
    Multiple Kernel Learning for one-class-classification.

    Learns a One-Class SVM classifier and its kernel weights. Makes only
    sense if multiple kernels are used.

    See:   CMKL

    C++ includes: MKLOneClass.h 
    """
    __swig_setmethods__ = {}
    for _s in [MKL]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, MKLOneClass, name, value)
    __swig_getmethods__ = {}
    for _s in [MKL]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, MKLOneClass, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self, s=None) -> MKLOneClass
        __init__(self) -> MKLOneClass

        Constructor

        Parameters:
        -----------

        s:  SVM to use as constraint generator in MKL SILP 
        """
        this = _Classifier.new_MKLOneClass(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_MKLOneClass
    __del__ = lambda self : None;
MKLOneClass_swigregister = _Classifier.MKLOneClass_swigregister
MKLOneClass_swigregister(MKLOneClass)

class MKLMultiClass(MultiClassSVM):
    """
    MKLMultiClass is a class for L1-norm multiclass MKL.

    It is based on the GMNPSVM Multiclass SVM. Its own parameters are the
    L2 norm weight change based MKL Its termination criterion set by void
    set_mkl_epsilon(float64_t eps ); and the maximal number of MKL
    iterations set by void set_max_num_mkliters(int32_t maxnum); It passes
    the regularization constants C1 and C2 to GMNPSVM.

    C++ includes: MKLMultiClass.h 
    """
    __swig_setmethods__ = {}
    for _s in [MultiClassSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, MKLMultiClass, name, value)
    __swig_getmethods__ = {}
    for _s in [MultiClassSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, MKLMultiClass, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> MKLMultiClass
        __init__(self, C, k, lab) -> MKLMultiClass

        Class Constructor commonly used in Shogun Toolbox

        Parameters:
        -----------

        C:  constant C

        k:  kernel

        lab:  labels 
        """
        this = _Classifier.new_MKLMultiClass(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_MKLMultiClass
    __del__ = lambda self : None;
    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train Multiclass MKL classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.MKLMultiClass_train(*args)

    def getsubkernelweights(*args):
        """
        getsubkernelweights(self, numweights) -> float

        returns MKL weights for the different kernels

        Parameters:
        -----------

        numweights:  is output parameter, is set to zero if no weights have
        been computed or to the number of MKL weights which is equal to the
        number of kernels

        NULL if no weights have been computed or otherwise an array with the
        weights, caller has to delete[] the output by itself 
        """
        return _Classifier.MKLMultiClass_getsubkernelweights(*args)

    def set_mkl_epsilon(*args):
        """
        set_mkl_epsilon(self, eps)

        sets MKL termination threshold

        Parameters:
        -----------

        eps:  is the desired threshold value the termination criterion is the
        L2 norm between the current MKL weights and their counterpart from the
        previous iteration 
        """
        return _Classifier.MKLMultiClass_set_mkl_epsilon(*args)

    def set_max_num_mkliters(*args):
        """
        set_max_num_mkliters(self, maxnum)

        sets maximal number of MKL iterations

        Parameters:
        -----------

        maxnum:  is the desired maximal number of MKL iterations; when it is
        reached the MKL terminates irrespective of the MKL progress set it to
        a nonpositive value in order to turn it off 
        """
        return _Classifier.MKLMultiClass_set_max_num_mkliters(*args)

MKLMultiClass_swigregister = _Classifier.MKLMultiClass_swigregister
MKLMultiClass_swigregister(MKLMultiClass)

class SVM(CSVM):
    def __init__(self, kernel, alphas, support_vectors, b):
        assert(len(alphas)==len(support_vectors))
        num_sv=len(alphas)
        CSVM.__init__(self, num_sv)
        self.set_alphas(alphas)
        self.set_support_vectors(support_vectors)
        self.set_kernel(kernel)
        self.set_bias(b)

DEF_PRECISION = _Classifier.DEF_PRECISION
class QP(_object):
    """Proxy of C++ QP class"""
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, QP, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda self, name: _swig_getattr(self, QP, name)
    __repr__ = _swig_repr
    __swig_setmethods__["opt_n"] = _Classifier.QP_opt_n_set
    __swig_getmethods__["opt_n"] = _Classifier.QP_opt_n_get
    if _newclass:opt_n = _swig_property(_Classifier.QP_opt_n_get, _Classifier.QP_opt_n_set)
    __swig_setmethods__["opt_m"] = _Classifier.QP_opt_m_set
    __swig_getmethods__["opt_m"] = _Classifier.QP_opt_m_get
    if _newclass:opt_m = _swig_property(_Classifier.QP_opt_m_get, _Classifier.QP_opt_m_set)
    __swig_setmethods__["opt_ce"] = _Classifier.QP_opt_ce_set
    __swig_getmethods__["opt_ce"] = _Classifier.QP_opt_ce_get
    if _newclass:opt_ce = _swig_property(_Classifier.QP_opt_ce_get, _Classifier.QP_opt_ce_set)
    __swig_setmethods__["opt_ce0"] = _Classifier.QP_opt_ce0_set
    __swig_getmethods__["opt_ce0"] = _Classifier.QP_opt_ce0_get
    if _newclass:opt_ce0 = _swig_property(_Classifier.QP_opt_ce0_get, _Classifier.QP_opt_ce0_set)
    __swig_setmethods__["opt_g"] = _Classifier.QP_opt_g_set
    __swig_getmethods__["opt_g"] = _Classifier.QP_opt_g_get
    if _newclass:opt_g = _swig_property(_Classifier.QP_opt_g_get, _Classifier.QP_opt_g_set)
    __swig_setmethods__["opt_g0"] = _Classifier.QP_opt_g0_set
    __swig_getmethods__["opt_g0"] = _Classifier.QP_opt_g0_get
    if _newclass:opt_g0 = _swig_property(_Classifier.QP_opt_g0_get, _Classifier.QP_opt_g0_set)
    __swig_setmethods__["opt_xinit"] = _Classifier.QP_opt_xinit_set
    __swig_getmethods__["opt_xinit"] = _Classifier.QP_opt_xinit_get
    if _newclass:opt_xinit = _swig_property(_Classifier.QP_opt_xinit_get, _Classifier.QP_opt_xinit_set)
    __swig_setmethods__["opt_low"] = _Classifier.QP_opt_low_set
    __swig_getmethods__["opt_low"] = _Classifier.QP_opt_low_get
    if _newclass:opt_low = _swig_property(_Classifier.QP_opt_low_get, _Classifier.QP_opt_low_set)
    __swig_setmethods__["opt_up"] = _Classifier.QP_opt_up_set
    __swig_getmethods__["opt_up"] = _Classifier.QP_opt_up_get
    if _newclass:opt_up = _swig_property(_Classifier.QP_opt_up_get, _Classifier.QP_opt_up_set)
    def __init__(self, *args): 
        """__init__(self) -> QP"""
        this = _Classifier.new_QP(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_QP
    __del__ = lambda self : None;
QP_swigregister = _Classifier.QP_swigregister
QP_swigregister(QP)

class SVMLight(CSVM):
    """Proxy of C++ SVMLight class"""
    __swig_setmethods__ = {}
    for _s in [CSVM]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, SVMLight, name, value)
    __swig_getmethods__ = {}
    for _s in [CSVM]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, SVMLight, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> SVMLight
        __init__(self, C, k, lab) -> SVMLight
        """
        this = _Classifier.new_SVMLight(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_SVMLight
    __del__ = lambda self : None;
    def init(*args):
        """init(self)"""
        return _Classifier.SVMLight_init(*args)

    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.SVMLight_train(*args)

    def get_runtime(*args):
        """get_runtime(self) ->  int"""
        return _Classifier.SVMLight_get_runtime(*args)

    def svm_learn(*args):
        """svm_learn(self)"""
        return _Classifier.SVMLight_svm_learn(*args)

    def optimize_to_convergence(*args):
        """
        optimize_to_convergence(self, docs, label, totdoc, shrink_state, inconsistent, a, 
            lin, c, timing_profile, maxdiff, heldout, retrain) ->  int
        """
        return _Classifier.SVMLight_optimize_to_convergence(*args)

    def compute_objective_function(*args):
        """compute_objective_function(self, a, lin, c, eps, label, totdoc) -> float"""
        return _Classifier.SVMLight_compute_objective_function(*args)

    def clear_index(*args):
        """clear_index(self, index)"""
        return _Classifier.SVMLight_clear_index(*args)

    def add_to_index(*args):
        """add_to_index(self, index, elem)"""
        return _Classifier.SVMLight_add_to_index(*args)

    def compute_index(*args):
        """compute_index(self, binfeature, range, index) ->  int"""
        return _Classifier.SVMLight_compute_index(*args)

    def optimize_svm(*args):
        """
        optimize_svm(self, docs, label, exclude_from_eq_const, eq_target, chosen, 
            active2dnum, totdoc, working2dnum, varnum, 
            a, lin, c, aicache, qp, epsilon_crit_target)
        """
        return _Classifier.SVMLight_optimize_svm(*args)

    def compute_matrices_for_optimization(*args):
        """
        compute_matrices_for_optimization(self, docs, label, exclude_from_eq_const, eq_target, chosen, 
            active2dnum, key, a, lin, c, varnum, totdoc, 
            aicache, qp)
        """
        return _Classifier.SVMLight_compute_matrices_for_optimization(*args)

    def compute_matrices_for_optimization_parallel(*args):
        """
        compute_matrices_for_optimization_parallel(self, docs, label, exclude_from_eq_const, eq_target, chosen, 
            active2dnum, key, a, lin, c, varnum, totdoc, 
            aicache, qp)
        """
        return _Classifier.SVMLight_compute_matrices_for_optimization_parallel(*args)

    def calculate_svm_model(*args):
        """calculate_svm_model(self, docs, label, lin, a, a_old, c, working2dnum, active2dnum) ->  int"""
        return _Classifier.SVMLight_calculate_svm_model(*args)

    def check_optimality(*args):
        """
        check_optimality(self, label, a, lin, c, totdoc, maxdiff, epsilon_crit_org, 
            misclassified, inconsistent, active2dnum, last_suboptimal_at, 
            iteration) ->  int
        """
        return _Classifier.SVMLight_check_optimality(*args)

    def update_linear_component(*args):
        """
        update_linear_component(self, docs, label, active2dnum, a, a_old, working2dnum, totdoc, 
            lin, aicache, c)
        """
        return _Classifier.SVMLight_update_linear_component(*args)

    def update_linear_component_mkl_linadd_helper(*args):
        """update_linear_component_mkl_linadd_helper(p) -> void"""
        return _Classifier.SVMLight_update_linear_component_mkl_linadd_helper(*args)

    if _newclass:update_linear_component_mkl_linadd_helper = staticmethod(update_linear_component_mkl_linadd_helper)
    __swig_getmethods__["update_linear_component_mkl_linadd_helper"] = lambda x: update_linear_component_mkl_linadd_helper
    def update_linear_component_mkl(*args):
        """
        update_linear_component_mkl(self, docs, label, active2dnum, a, a_old, working2dnum, totdoc, 
            lin, aicache)
        """
        return _Classifier.SVMLight_update_linear_component_mkl(*args)

    def update_linear_component_mkl_linadd(*args):
        """
        update_linear_component_mkl_linadd(self, docs, label, active2dnum, a, a_old, working2dnum, totdoc, 
            lin, aicache)
        """
        return _Classifier.SVMLight_update_linear_component_mkl_linadd(*args)

    def call_mkl_callback(*args):
        """call_mkl_callback(self, a, label, lin)"""
        return _Classifier.SVMLight_call_mkl_callback(*args)

    def select_next_qp_subproblem_grad(*args):
        """
        select_next_qp_subproblem_grad(self, label, a, lin, c, totdoc, qp_size, inconsistent, active2dnum, 
            working2dnum, selcrit, select, cache_only, 
            key, chosen) ->  int
        """
        return _Classifier.SVMLight_select_next_qp_subproblem_grad(*args)

    def select_next_qp_subproblem_rand(*args):
        """
        select_next_qp_subproblem_rand(self, label, a, lin, c, totdoc, qp_size, inconsistent, active2dnum, 
            working2dnum, selcrit, select, key, 
            chosen, iteration) ->  int
        """
        return _Classifier.SVMLight_select_next_qp_subproblem_rand(*args)

    def select_top_n(*args):
        """select_top_n(self, selcrit, range, select, n)"""
        return _Classifier.SVMLight_select_top_n(*args)

    def init_shrink_state(*args):
        """init_shrink_state(self, shrink_state, totdoc, maxhistory)"""
        return _Classifier.SVMLight_init_shrink_state(*args)

    def shrink_state_cleanup(*args):
        """shrink_state_cleanup(self, shrink_state)"""
        return _Classifier.SVMLight_shrink_state_cleanup(*args)

    def shrink_problem(*args):
        """
        shrink_problem(self, shrink_state, active2dnum, last_suboptimal_at, iteration, 
            totdoc, minshrink, a, inconsistent, c, 
            lin, label) ->  int
        """
        return _Classifier.SVMLight_shrink_problem(*args)

    def reactivate_inactive_examples(*args):
        """
        reactivate_inactive_examples(self, label, a, shrink_state, lin, c, totdoc, iteration, 
            inconsistent, docs, aicache, maxdiff)
        """
        return _Classifier.SVMLight_reactivate_inactive_examples(*args)

SVMLight_swigregister = _Classifier.SVMLight_swigregister
SVMLight_swigregister(SVMLight)

def SVMLight_update_linear_component_mkl_linadd_helper(*args):
  """SVMLight_update_linear_component_mkl_linadd_helper(p) -> void"""
  return _Classifier.SVMLight_update_linear_component_mkl_linadd_helper(*args)

class DomainAdaptationSVM(SVMLight):
    """
    class DomainAdaptiveSVM

    C++ includes: DomainAdaptationSVM.h 
    """
    __swig_setmethods__ = {}
    for _s in [SVMLight]: __swig_setmethods__.update(getattr(_s,'__swig_setmethods__',{}))
    __setattr__ = lambda self, name, value: _swig_setattr(self, DomainAdaptationSVM, name, value)
    __swig_getmethods__ = {}
    for _s in [SVMLight]: __swig_getmethods__.update(getattr(_s,'__swig_getmethods__',{}))
    __getattr__ = lambda self, name: _swig_getattr(self, DomainAdaptationSVM, name)
    __repr__ = _swig_repr
    def __init__(self, *args): 
        """
        __init__(self) -> DomainAdaptationSVM
        __init__(self, C, k, lab, presvm, B) -> DomainAdaptationSVM

        constructor

        Parameters:
        -----------

        C:  cost constant C

        k:  kernel

        lab:  labels

        presvm:  trained SVM to regularize against

        B:  trade-off constant B 
        """
        this = _Classifier.new_DomainAdaptationSVM(*args)
        try: self.this.append(this)
        except: self.this = this
    __swig_destroy__ = _Classifier.delete_DomainAdaptationSVM
    __del__ = lambda self : None;
    def init(*args):
        """
        init(self, presvm, B)

        init SVM

        Parameters:
        -----------

        presvm:  trained SVM to regularize against

        B:  trade-off constant B 
        """
        return _Classifier.DomainAdaptationSVM_init(*args)

    def train(*args):
        """
        train(self, data=None) -> bool
        train(self) -> bool

        train SVM classifier

        Parameters:
        -----------

        data:  training data (parameter can be avoided if distance or kernel-
        based classifiers are used and distance/kernels are initialized with
        train data)

        whether training was successful 
        """
        return _Classifier.DomainAdaptationSVM_train(*args)

    def get_presvm(*args):
        """
        get_presvm(self) -> CSVM

        returns SVM that is used as prior information

        presvm 
        """
        return _Classifier.DomainAdaptationSVM_get_presvm(*args)

    def get_B(*args):
        """
        get_B(self) -> float

        getter for regularization parameter B

        regularization parameter B 
        """
        return _Classifier.DomainAdaptationSVM_get_B(*args)

DomainAdaptationSVM_swigregister = _Classifier.DomainAdaptationSVM_swigregister
DomainAdaptationSVM_swigregister(DomainAdaptationSVM)



